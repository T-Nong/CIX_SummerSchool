{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_XGVwCn-Qgn"
      },
      "source": [
        "<img src=\"images\\header_CIX2025.png\" alt=\"CIX 2025\" width=\"1200\"/>\n",
        "\n",
        "Toan Nong (post-doc at CNRS, Marc Jeannerod Institute, Dreher lab, Lyon): tnong@isc.cnrs.fr \n",
        "\n",
        "\n",
        "# Theory-of-Mind (ToM) models in games\n",
        "\n",
        "This course is designed for participants with a range of backgrounds. You do not need advanced programming or neuroscience experience—key concepts will be explained along the way.\n",
        "\n",
        "## Learning Objectives:\n",
        "By the end of this course, participants will be able to:\n",
        "\n",
        "1. Understand Theory-of-Mind (ToM) Concepts\n",
        "- Explain the concept of Theory-of-Mind and its relevance to strategic decision-making in games.\n",
        "- Differentiate between basic representation and meta-representation in social cognition.\n",
        "\n",
        "2. Describe and Compare Decision Models in Social Games\n",
        "- Summarize the principles behind Q-Learning, Fictitious Learning, Influence Learning, and Mixed-Intentions Influence Learning (MIIL) models.\n",
        "- Compare how different models account for social reasoning and adaptation in agent behavior.\n",
        "\n",
        "3. Implement and Simulate Cognitive Agents\n",
        "- Use Python and provided code templates to implement and simulate agents using different ToM-related learning models.\n",
        "- Modify simulation parameters to observe and interpret the effects on agent behavior and outcomes.\n",
        "\n",
        "4. Analyze and Interpret Simulation Results\n",
        "- Visualize agent actions and rewards over time using data analysis libraries (e.g., pandas, seaborn, matplotlib).\n",
        "- Draw conclusions from simulation outcomes to better understand model properties and agent interactions.\n",
        "\n",
        "5. Critically Evaluate Model Assumptions and Limitations\n",
        "- Reflect on the suitability and limitations of each model in capturing real-world social decision-making.\n",
        "- Propose potential extensions or alternative approaches for modeling ToM in games.\n",
        "\n",
        "6. Grasp key concepts behind the use of Computational Modeling\n",
        "- understand how to fit and compare different computational models\n",
        "- learn good practices and check models validity with sanity checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## General introduction to ToM\n",
        "Trying to read minds, so wondering what and how other people think is what we call Theory-of-Mind (ToM). \n",
        "More precisely, it is defined as: “The capacity to represent that and how agents represent the world from their own points of view (meta-representation).” (Rakoczy, Nat. Rev. Psy. 2022).\n",
        "A simple representation would include only the basic characteristics of others, while meta-representation, so ToM, would incorporate the representation of others’ representation. \n",
        "\n",
        "<img src=\"images\\Basic_vs_meta_representation.jpg\" alt=\"Basic vs Meta representation\" width=\"1000\"/>\n",
        "\n",
        "To study ToM, we can think on 3 different levels, sometimes referred to as Marr’s framework. The first level to understand is the *goal* of the social behaviour that we want to study, here Theory of mind (or *why* ToM is used). This requires to understand the context in which we study ToM (competitive/cooperative/a mixed game? Dyadic/Fixed group/Dynamic network  interaction? Etc...) and its subcomponents (Beliefs? Intentions? Reasoning? Emotions? Etc...) that we want to study. The 2nd level asks what algorithms are applied when ToM is deployed in a given context and for specific subcomponents. This would require to specify the computational models of the different ToM subcomponents. The last implementational level asks how ToM subcomponents are physically encoded. This requires to study the brain networks, areas, cells, etc. that lead to the emergence of the 2 first levels of ToM. We will here focus on the algorithmic level.\n",
        "\n",
        "<img src=\"images\\ToM_Marrs_levels.jpg\" alt=\"ToM Marr's levels\" width=\"600\"/>\n",
        "\n",
        "More precisely, we will only focus on some specific ToM subcomponents used in strategic games that have been well studied on the 3 different Marr’s levels. To present the first one, imagine the following situation:\n",
        "\n",
        "A lazy worker and an inspector. The inspector’s aim is that the worker works without being behind his back all the time. If the worker slacks off too much, the inspector will come more often. If the worker works enough, the inspector will come less often. In this case, the worker’s action has an influence on the inspector’s action (and vice-versa). This exemple shows how representing the influence of one’s action on the other’s decision is useful. See also Hampton et al., PNAS 2008.\n",
        "\n",
        "As for the 2nd subcomponent that we will study, think about diplomatic relations between neighbouring countries. They must always be on the watch for the intentions of the other countries that can change suddenly. This would require the ability to adapt to non-signaled changes of intentions. Another example would be unknown agents in a network: these agents could pretend to have good intentions. But they might switch to malevolent actions suddenly. If they are caught and rehabilitated, they might go back to behave as \"good\" agents. But they might still be tempted to act badly in the future.\n",
        "\n",
        "<img src=\"images\\ToM_subcomponents_in_strategic_games.jpg\" alt=\"ToM subcomponents in strategic games\" width=\"800\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models of decisions in social strategic games\n",
        "\n",
        "The question now is: How can we model these ToM subcomponents? We will see mainly 4 decision models usable in strategic games: the Q-Learning (QL), Fictitious Play/Fictitious Learning (FP), Influence and Mixed-Intentions Influence Learning (MIIL) models.\n",
        "\n",
        "### Q-Learning\n",
        "\n",
        "**Real-World Analogy**:\n",
        "\n",
        "Imagine teaching a dog to fetch a stick. The dog tries different things—running, sniffing, barking. Whenever the dog successfully fetches the stick, you reward it with a treat. Over time, the dog learns which actions lead to treats and repeats those, ignoring the rest. Q-Learning works similarly: an agent tries various actions and learns, through feedback (rewards), which actions are best.\n",
        "\n",
        "Q-learning is a **model-free reinforcement learning algorithm**.  \n",
        "Agents update their estimates of action values based on **received rewards**.\n",
        "\n",
        "**Core Idea**:  \n",
        "The agent maintains a value function $Q(a)$ for each action $a$, updated as:\n",
        "\n",
        "$$\n",
        "Q(a_t) \\leftarrow Q(a_t) + \\alpha \\cdot (r_t - Q(a_t))\n",
        "$$\n",
        "\n",
        "- $ \\alpha $: learning rate (0 < α ≤ 1)  \n",
        "- $ r_t $: reward received at time $t$   \n",
        "- $ a_t $: action taken at time $t$ \n",
        "\n",
        "**Action selection** uses a softmax policy:\n",
        "\n",
        "$$\n",
        "P(a_t) = \\frac{e^{\\beta Q(a_t)}}{\\sum_{a'} e^{\\beta Q(a')}}\n",
        "$$\n",
        "\n",
        "- $\\beta$ : inverse temperature controlling exploration vs. exploitation\n",
        "\n",
        "### Fictitious Learning\n",
        "\n",
        "**Real-World Analogy**:\n",
        "\n",
        "Suppose you’re playing rock-paper-scissors with a friend. Even when you pick “rock” and lose, you might still think about what would have happened if you’d chosen “paper” or “scissors” instead. Fictitious learning is about using not just what actually happened, but also what could have happened, to learn faster and more flexibly.\n",
        "\n",
        "It is a **value-based learning approach** where agents update their estimates of **all actions**, not just the one chosen.  \n",
        "It blends real experience with **counterfactual (fictitious)** reasoning to estimate what would have happened if a different action had been taken.\n",
        "\n",
        "**Core Idea**:  \n",
        "The agent maintains a value function $Q(a)$ for each action $a$, and updates **both** chosen and unchosen actions:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Q(a_t) &\\leftarrow Q(a_t) + \\alpha \\cdot (r_t - Q(a_t)) \\quad &&\\text{(chosen action)} \\\\\\\\\n",
        "Q(a_{\\neg t}) &\\leftarrow Q(a_{\\neg t}) + \\alpha \\cdot (\\hat{r}_{\\neg t} - Q(a_{\\neg t})) \\quad &&\\text{(unchosen action)}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "- $\\alpha$: learning rate (0 < α ≤ 1)  \n",
        "- $r_t$: actual reward received for action $a_t$\n",
        "- $\\hat{r}_{\\neg t}$: **fictitious reward** for unchosen action (e.g., expected or modeled)\n",
        "\n",
        "**Action selection** uses a softmax policy:\n",
        "\n",
        "$$\n",
        "P(a_t) = \\frac{e^{\\beta Q(a_t)}}{\\sum_{a'} e^{\\beta Q(a')}}\n",
        "$$\n",
        "\n",
        "- $\\beta$: inverse temperature controlling exploration vs. exploitation  \n",
        "- Higher $\\beta$ makes the policy more deterministic (greedy), while lower $\\beta$ increases exploration.\n",
        "\n",
        "Fictitious learning is commonly applied in **model-based reinforcement learning**, **cognitive modeling**, and **social learning**, where agents reason beyond direct experience.\n",
        "\n",
        "### Influence Learning Model\n",
        "\n",
        "**Real-World Analogy**:\n",
        "\n",
        "Imagine two chess players. One notices that their moves seem to influence their opponent’s strategy. The player starts to pick moves not just for their own benefit, but also to shape how the opponent will play in the future. Influence learning captures this recursive, “they know that I know” reasoning. In this sense, Influence learning adds a **recursive social component** (ToM).\n",
        "\n",
        "**Core Idea**:  \n",
        "The agent believes the opponent is learning from them. It adjusts its own action values based on **how it thinks its own past actions influenced the opponent**.\n",
        "\n",
        "It introduces a second-order update via an **influence term** $I_t(a)$:\n",
        "\n",
        "$$\n",
        "Q(a_t) \\leftarrow Q(a_t) + \\eta \\cdot (r_t - Q(a_t)) + \\lambda \\cdot I_t(a_t)\n",
        "$$\n",
        "\n",
        "- $\\eta$: self learning rate  \n",
        "- $\\lambda$: influence sensitivity  \n",
        "- $I_t(a)$: estimated influence of agent's action on opponent's future behavior\n",
        "\n",
        "Still uses softmax to pick actions:\n",
        "\n",
        "$$\n",
        "P(a_t) = \\frac{e^{\\beta Q(a_t)}}{\\sum_{a'} e^{\\beta Q(a')}}\n",
        "$$\n",
        "\n",
        "\n",
        "### Mixed-Intentions Influence Learning (MIIL) Model\n",
        "\n",
        "**Real-World Analogy**:\n",
        "\n",
        "Imagine two countries, Country A and Country B, interact over time. Sometimes they are in a cooperative phase (working together on trade or climate), sometimes in a competitive phase (disputes over borders or resources)—but it’s not always clear which situation they are in at any moment.\n",
        "\n",
        "Country A has to:\n",
        "- Guess whether the current interaction is cooperative or competitive, based on how Country B behaves.\n",
        "- Predict how Country B will act in both scenarios (e.g., will they honor a trade deal if it’s cooperation, or try to outmaneuver in competition?).\n",
        "- Notice if its own diplomatic actions (offers, threats, alliances) influence how Country B responds in the future.\n",
        "- Decide on its next move (cooperate, negotiate, escalate, etc.), blending what it knows about both possible situations, weighted by how likely each is.\n",
        "\n",
        "The MIIL model is like Country A’s diplomatic “playbook,” constantly updating its beliefs about the situation, its opponent’s tendencies, and the effect of its own strategies—so it can adapt and thrive in a complex, changing world.\n",
        "\n",
        "**Core Idea**:\n",
        "\n",
        "The **MIIL model** (Philippe et al., 2024, *Nature Communications*) is a computational model designed for agents interacting across **two types of games** (e.g., coordination and competition). It captures a mix of **belief learning** and **influence learning**, under uncertainty about the opponent's intentions.\n",
        "\n",
        "---\n",
        "\n",
        "#### Hidden States\n",
        "\n",
        "MIIL uses **three internal beliefs**, represented in log-odds (inverse sigmoid) space:\n",
        "\n",
        "- $x_0$: belief about opponent's behavior in Game 1\n",
        "- $x_1$: belief about opponent's behavior in Game 2\n",
        "- $x_2$: belief about **which game** is being played\n",
        "\n",
        "These are updated based on prediction errors at two levels.\n",
        "\n",
        "---\n",
        "\n",
        "#### Update Equations\n",
        "\n",
        "The MIIL update rule combines first-order belief learning and second-order influence learning.\n",
        "\n",
        "Let:\n",
        "- $P(o=1 \\mid \\text{Game 1}) = \\sigma(x_0)$\n",
        "- $P(o=1 \\mid \\text{Game 2}) = \\sigma(x_1)$\n",
        "- $P(\\text{Game 1}) = \\sigma(x_2)$\n",
        "\n",
        "The agent observes:\n",
        "- Opponent’s action $o$\n",
        "- Its own action $a$\n",
        "\n",
        "**Prediction errors**:\n",
        "- First-order PE: discrepancy between $o$ and prior beliefs\n",
        "- Second-order PE: discrepancy between agent's own action and what it believes the opponent expects from them\n",
        "\n",
        "These are combined using learning rates $\\eta$ (belief update) and $\\lambda$ (influence learning).\n",
        "\n",
        "The update uses:\n",
        "$$\n",
        "p_\\text{game1}' = p_\\text{game1} + \\eta \\cdot PE_1^\\text{game1} + \\lambda \\cdot \\text{InfluenceTerm}_1\n",
        "$$\n",
        "and similarly for game 2. The belief about the game is updated via a softmax-like transformation of the two internal beliefs.\n",
        "\n",
        "---\n",
        "\n",
        "#### Decision Rule\n",
        "\n",
        "The agent chooses its action probabilistically based on a **mixture of decision values** from both games:\n",
        "\n",
        "$$\n",
        "P(a = 1) = \\sigma\\left( P_\\text{game} \\cdot DV_\\text{game1} + (1 - P_\\text{game}) \\cdot DV_\\text{game2} \\right)\n",
        "$$\n",
        "\n",
        "Where $DV$ (decision value) for each game depends on the expected reward given the belief about the opponent.\n",
        "\n",
        "---\n",
        "\n",
        "#### Parameters\n",
        "\n",
        "The MIIL model includes:\n",
        "- $\\eta$: belief learning rate\n",
        "- $\\lambda$: influence learning rate\n",
        "- $\\beta$: precision of the softmax choice\n",
        "- slope and bias: for the belief about game identity\n",
        "\n",
        "---\n",
        "\n",
        "#### Key Features\n",
        "\n",
        "- Tracks **multiple intentions** across different game contexts.\n",
        "- Updates beliefs about **how the opponent reacts to one's own actions**.\n",
        "- Learns which type of game is likely being played.\n",
        "\n",
        "---\n",
        "\n",
        "In summary:\n",
        "\n",
        "<img src=\"images\\Models_general_presentation.jpg\" alt=\"ToM models general description\" width=\"1200\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rai1JqCl92hS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from scipy.special import digamma\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "from collections import defaultdict\n",
        "from scipy.stats import beta as beta_dist, gamma as gamma_dist\n",
        "\n",
        "# Importing the custom module for ToM functions\n",
        "from CIX_course_Modelling_ToM_functions import *\n",
        "from CIX_course_Fitting_ToM_functions import *\n",
        "from CIX_course_Sanity_checks_functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-EgkqUxQAMP"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility (optional)\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "pyro.set_rng_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWyU6PEzBOtA"
      },
      "source": [
        "## Simulation of decision agents with or without ToM subcomponents\n",
        "\n",
        "### Models structure\n",
        "The models have been implemented in CIX_course_Modelling_ToM_functions.py. The models are structured into an update (or evolution) function $f(u_t,\\theta)$ and a decision fonction $g(x_t,\\phi)$. As its names suggests, the update function updates the hidden states $x_t$ of the model and will need as inputs: the data $u_t$, the learning parameter(s) of the evolution function $\\theta$. As for the decision function, it returns the decision of the model $y_t$ based on the hidden states $x_t$ and the learning parameter(s) of the decision function $\\phi$. These functions sometimes also take hyperparameters as inputs, i.e. parameters that can configure part of the model's learning process, but we will not cover this today.\n",
        "\n",
        "<img src=\"images\\Decision_Model_structure.png\" alt=\"Models strucure\" width=\"1000\"/>\n",
        "\n",
        "You won't need to modify the following coding block. It is the main simulation loop function, and the initialization of the initial hidden states of a specific Mixed Artificial Agent (Mixed_AA), not to be mistaken with a MIIL agent. This agent switches between competitive and cooperative modes with a specific decision process described in Philippe et al. 2024 (see also f_Mixed_AA and g_Mixed_AA). Yo make it simple, it computes the probability that the other player chooses an action based on the outcomes and choices from the 2 previous trials, and either tries to coordinate (cooperative mode), or not to coordinate (competitive mode). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kfLfb7FrBREu"
      },
      "outputs": [],
      "source": [
        "# /!\\ Don't modify this cell\n",
        "\n",
        "# This function simulates two agents interacting over multiple rounds (trials).\n",
        "# Why? It allows us to observe how decision-making and learning evolve as each agent adapts to the other's behavior.\n",
        "def simulate_agents(agent1, agent2, n_trials=260):\n",
        "    history = []  # This will store all relevant info about every trial\n",
        "    for t in range(n_trials):\n",
        "        # Each agent selects an action for this trial, possibly using its current beliefs or strategy.\n",
        "        a1 = agent1.choose_action(t)\n",
        "        a2 = agent2.choose_action(t)\n",
        "\n",
        "        # After seeing each other's actions, both agents update their internal states (learning from experience).\n",
        "        # This step is crucial: it models how real-world learners adapt based on outcomes.\n",
        "        agent1.update(other_action=a2, own_action=a1, t=t)\n",
        "        agent2.update(other_action=a1, own_action=a2, t=t)\n",
        "\n",
        "        # We log the trial number, actions, and rewards for both agents.\n",
        "        # Why? This history lets us later analyze how strategies and payoffs change over time.\n",
        "        history.append({\n",
        "            \"trial_nb\": t + 1,\n",
        "            \"agent1_action\": a1,\n",
        "            \"agent2_action\": a2,\n",
        "            \"agent1_reward\": agent1.history[-1][2],\n",
        "            \"agent2_reward\": agent2.history[-1][2]\n",
        "        })\n",
        "    return history\n",
        "\n",
        "# This dictionary initializes the hidden states for a special \"Mixed Artificial Agent.\"\n",
        "# Why so many entries? The agent's strategy depends on recent history (last two rounds), so we keep track of all possibilities.\n",
        "x_init_AA = {(\"chose_0\",\"0000\"): 4, (\"chose_0\",\"0001\"): 4, (\"chose_0\",\"0010\"): 4, (\"chose_0\",\"0011\"): 4,\n",
        "             (\"chose_0\",\"0100\"): 4, (\"chose_0\",\"0101\"): 4, (\"chose_0\",\"0110\"): 4, (\"chose_0\",\"0111\"): 4,\n",
        "             (\"chose_0\",\"1000\"): 4, (\"chose_0\",\"1001\"): 4, (\"chose_0\",\"1010\"): 4, (\"chose_0\",\"1011\"): 4,\n",
        "             (\"chose_0\",\"1100\"): 4, (\"chose_0\",\"1101\"): 4, (\"chose_0\",\"1110\"): 4, (\"chose_0\",\"1111\"): 4,\n",
        "             (\"occ\",\"0000\"): 8, (\"occ\",\"0001\"): 8, (\"occ\",\"0010\"): 8, (\"occ\",\"0011\"): 8,\n",
        "             (\"occ\",\"0100\"): 8, (\"occ\",\"0101\"): 8, (\"occ\",\"0110\"): 8, (\"occ\",\"0111\"): 8,\n",
        "             (\"occ\",\"1000\"): 8, (\"occ\",\"1001\"): 8, (\"occ\",\"1010\"): 8, (\"occ\",\"1011\"): 8,\n",
        "             (\"occ\",\"1100\"): 8, (\"occ\",\"1101\"): 8, (\"occ\",\"1110\"): 8, (\"occ\",\"1111\"): 8}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e26_hvgYBfUw"
      },
      "source": [
        "### Context\n",
        "We will simulate a dyadic interaction between 2 players that can choose between 2 actions. The agents will be rewarded depending on the combination of the 2 players' actions. This will be summed up in a 2x2x2 payoff matrix. We will first consider simple games: the Hide-and-Seek (HaS) and the Coordination Game (CG). In the HaS, the Hider has to hide in one of two locations to avoid the Seeker, while the Seeker has to guess where the Hider is. In the CG, players have to coordinate on the same action to be rewarded. The HaS is a competitive game while the CG is a cooperative game. We will consider situations where the game is fixed, but also situations where the game can (unexpectedly) change. First, we will initialize some simulations variables that will be needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMVrjHdHBijx"
      },
      "outputs": [],
      "source": [
        "# Here we define the rewards for each combination of actions in two classic games.\n",
        "# Why? The payoff matrix defines the rules of the game—how much each agent earns for each action pair.\n",
        "payoff_matrix_HaS = np.zeros((2,2,2))\n",
        "payoff_matrix_HaS[:,:,0] = [[1, 0], [0, 1]]  # Player 1 gets a reward for \"matching\" in this role\n",
        "payoff_matrix_HaS[:,:,1] = [[0, 1], [1, 0]]  # Player 2 gets a reward for \"not matching\"\n",
        "\n",
        "payoff_matrix_CG = np.zeros((2,2,2))\n",
        "payoff_matrix_CG[:,:,0] = [[1, 0], [0, 1]]  # Coordination Game: both players benefit from coordinating\n",
        "payoff_matrix_CG[:,:,1] = [[1, 0], [0, 1]]\n",
        "\n",
        "n_trials = 100  # Number of rounds to simulate. Why? More trials = more learning and adaptation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's first simulate 2 QL agents playing with each other at CG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We initialize two Q-learning agents with random Q-values and parameters. Q-values represent how much reward the agent expects from each action.\n",
        "# Why random? It models initial uncertainty and allows us to see learning from scratch.\n",
        "\n",
        "# Agent 1\n",
        "x_init_q1 = np.random.randn(2)      # Start with random values for each possible action\n",
        "phi_q1 = np.random.randn(2)         # Random softmax \"temperature\" and bias\n",
        "theta_q1 = np.random.randn(1)       # Random learning rate (in log-odds space)\n",
        "\n",
        "# Agent 2 (same logic)\n",
        "x_init_q2 = np.random.randn(2)\n",
        "phi_q2 = np.random.randn(2)\n",
        "theta_q2 = np.random.randn(1)\n",
        "\n",
        "# Create agent objects. Why specify the game and player_id? So each agent knows the rules and its own role.\n",
        "agent1_QL = Agent(f_func=f_Qlearning, g_func=g_Qlearning, x_init=x_init_q1, phi=phi_q1, theta=theta_q1, game=payoff_matrix_CG, player_id=1)\n",
        "agent2_QL = Agent(f_func=f_Qlearning, g_func=g_Qlearning, x_init=x_init_q2, phi=phi_q2, theta=theta_q2, game=payoff_matrix_CG, player_id=2)\n",
        "\n",
        "# Simulate the agents playing together and collect the results.\n",
        "history_QL = simulate_agents(agent1_QL, agent2_QL, n_trials=n_trials)\n",
        "\n",
        "# Convert the results into a DataFrame for easier analysis and plotting.\n",
        "history_df_QL = pd.DataFrame(history_QL)\n",
        "\n",
        "# Plot the actions taken by both agents over time.\n",
        "# Why? This visualization helps us see patterns—are they learning to coordinate? Are their choices random?\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(25, 6))\n",
        "sns.pointplot(data=history_df_QL, x=\"trial_nb\", y=\"agent1_action\", label=\"Agent 1 Action\", color=\"blue\")\n",
        "sns.pointplot(data=history_df_QL, x=\"trial_nb\", y=\"agent2_action\", label=\"Agent 2 Action\", color=\"orange\")\n",
        "plt.title(\"Actions of Q-learning Agents Over Trials (CG)\")\n",
        "plt.xlabel(\"Trial Number\")\n",
        "plt.ylabel(\"Action (0 or 1)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Similarly, plot the rewards earned by each agent over time.\n",
        "# Why? To see if agents are learning to maximize their payoffs.\n",
        "plt.figure(figsize=(25, 6))\n",
        "sns.pointplot(data=history_df_QL, x=\"trial_nb\", y=\"agent1_reward\", label=\"Agent 1 Reward\", color=\"blue\")\n",
        "sns.pointplot(data=history_df_QL, x=\"trial_nb\", y=\"agent2_reward\", label=\"Agent 2 Reward\", color=\"orange\")\n",
        "plt.title(\"Rewards of Q-learning Agents Over Trials (CG)\")\n",
        "plt.xlabel(\"Trial Number\")  \n",
        "plt.ylabel(\"Reward\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.1.a) Now do the same for the HaS.\n",
        "\n",
        "Qu.1.b) Simulate games for different agents in the HaS and CG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's make a QL agent play with a Mixed AA that switches between competitive and cooperative modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We want to test how a QL agent interacts with a \"Mixed Artificial Agent\" that sometimes cooperates and sometimes competes.\n",
        "\n",
        "# First, we create a schedule: which trials use Game 1 (cooperation) and which use Game 2 (competition)?\n",
        "block_size = 20  # Each block of trials will be 20 rounds\n",
        "game1_size_in_blocks = 12  # Of those, the first 12 are Game 1\n",
        "\n",
        "game1_trials = np.zeros(n_trials, dtype=int)\n",
        "for start in range(0, n_trials, block_size):\n",
        "    # The first part of each block is \"Game 1\"\n",
        "    game1_trials[start:start + game1_size_in_blocks] = 1\n",
        "game2_trials = 1 - game1_trials  # The rest are \"Game 2\" (opposite)\n",
        "\n",
        "print(\"Game 1 trials:\", game1_trials)\n",
        "print(\"Game 2 trials:\", game2_trials)\n",
        "\n",
        "# Initialize a QL agent (as above)\n",
        "x_init_q1 = np.random.randn(2)\n",
        "phi_q1 = np.random.randn(2)\n",
        "theta_q1 = np.random.randn(1)\n",
        "agent1_QL = Agent(f_func=f_Qlearning, g_func=g_Qlearning, x_init=x_init_q1, phi=phi_q1, theta=theta_q1, game=payoff_matrix_CG, player_id=1)\n",
        "\n",
        "# Create a Mixed Artificial Agent that switches mode based on the current trial's game.\n",
        "# This agent uses the outcome of recent rounds to decide whether to coordinate or compete.\n",
        "agent2_Mixed_AA = MixedAgent(\n",
        "    f_func=f_Mixed_AA,\n",
        "    g_func=g_Mixed_AA,\n",
        "    x_init=x_init_AA,\n",
        "    phi=None,\n",
        "    theta=None,\n",
        "    game1=payoff_matrix_CG,\n",
        "    game2=payoff_matrix_HaS,\n",
        "    game1_trials=game1_trials,\n",
        "    game2_trials=game2_trials,\n",
        "    player_id=2\n",
        ")\n",
        "\n",
        "# Simulate their interaction.\n",
        "history_QL_vs_Mixed_AA = simulate_agents(agent1_QL, agent2_Mixed_AA, n_trials=n_trials)\n",
        "history_df_QL_vs_Mixed_AA = pd.DataFrame(history_QL_vs_Mixed_AA)\n",
        "\n",
        "# Plot results as before.\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(25, 6))\n",
        "sns.pointplot(data=history_df_QL_vs_Mixed_AA, x=\"trial_nb\", y=\"agent1_action\", label=\"Agent 1 Action\", color=\"blue\")\n",
        "sns.pointplot(data=history_df_QL_vs_Mixed_AA, x=\"trial_nb\", y=\"agent2_action\", label=\"Agent 2 Action\", color=\"orange\")\n",
        "plt.title(\"Actions of QL (Agent1) and Mixed AA (Agent2) Over Trials\")\n",
        "plt.xlabel(\"Trial Number\")\n",
        "plt.ylabel(\"Action (0 or 1)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(25, 6))\n",
        "sns.pointplot(data=history_df_QL_vs_Mixed_AA, x=\"trial_nb\", y=\"agent1_reward\", label=\"Agent 1 Reward\", color=\"blue\")\n",
        "sns.pointplot(data=history_df_QL_vs_Mixed_AA, x=\"trial_nb\", y=\"agent2_reward\", label=\"Agent 2 Reward\", color=\"orange\")\n",
        "plt.title(\"Rewards of QL (Agent1) and Mixed AA (Agent2) Over Trials\")\n",
        "plt.xlabel(\"Trial Number\")  \n",
        "plt.ylabel(\"Reward\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.2.a) What can you observe for the rewards of the different agents? \n",
        "\n",
        "Qu.2.b) Simulate a game between a MIIL agent vs a Mixed AA.\n",
        "\n",
        "Qu.2.c) What can you observe for the rewards of the different agents?\n",
        "\n",
        "Qu.2.d) What happens if you simulate longer games with more trials? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8jddsc0CHVB"
      },
      "source": [
        "Let's now run multiple game simulations and look at the average reward for each model against any other given model. We want to see if there is an advantage for any model against the others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_sim = 100          # Number of full simulations to run (to get robust statistics, not just one random run)\n",
        "n_trials = 300        # Number of trials (rounds) in each simulation. Why? Longer games allow more learning and pattern observation.\n",
        "\n",
        "# We want to simulate agents that switch between two types of games (e.g. cooperative and competitive).\n",
        "# To do this, we need to determine for each trial which game is being played.\n",
        "\n",
        "block_size = 20            # Each \"block\" of trials consists of 20 consecutive rounds.\n",
        "game1_size_in_blocks = 12  # In each block, the first 12 trials are Game 1 (e.g., cooperation), the remainder are Game 2.\n",
        "\n",
        "# Create an array to flag which trials are Game 1 (1) or not (0).\n",
        "game1_trials = np.zeros(n_trials, dtype=int)  # Start with all trials as Game 2 (0).\n",
        "for start in range(0, n_trials, block_size):\n",
        "    # For each block, mark the first 'game1_size_in_blocks' trials as Game 1 (set to 1).\n",
        "    # Why? This simulates a structured environment where agents face different games in predictable chunks.\n",
        "    game1_trials[start:start + game1_size_in_blocks] = 1\n",
        "\n",
        "# Game 2 is just the complement of Game 1 (if not Game 1, it's Game 2).\n",
        "game2_trials = 1 - game1_trials\n",
        "\n",
        "# Print arrays to visually verify game switching schedule (helps with debugging and understanding).\n",
        "print(\"Game 1 trials:\", game1_trials)\n",
        "print(\"Game 2 trials:\", game2_trials)\n",
        "\n",
        "full_sims_hist = []  # This will store the results (histories) of all simulations for later analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.3.a) Fill the following block of code to run nb_sim simulations of different agents playing against each others.\n",
        "\n",
        "Qu.3.b) Is there any advantage of using one model vs another? Is ToM always useful? How come? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v9cpwj4CMIg"
      },
      "outputs": [],
      "source": [
        "# Loop through the number of simulations\n",
        "for \n",
        "    # Example for Q-learning agent\n",
        "    x_init_q =      \n",
        "    phi_q =       \n",
        "    theta_q =     \n",
        "\n",
        "    # Example for Fictitious learner\n",
        "    x_init_FP = \n",
        "    phi_FP = \n",
        "    theta_FP = \n",
        "\n",
        "    # Example for Influence learner\n",
        "    x_init_inf = \n",
        "    phi_inf = \n",
        "    theta_inf = \n",
        "\n",
        "    # Example for MIIL (Mixed-Intentions Influence Learner)\n",
        "    x_init_miil = \n",
        "    phi_miil = \n",
        "    theta_miil = \n",
        "\n",
        "    # --- Initialize agent objects for player 1 ---\n",
        "    agent1_QL = \n",
        "    agent1_FP = \n",
        "    agent1_Inf = \n",
        "    agent1_MIIL = \n",
        "\n",
        "    # --- Initialize agent objects for player 2 ---\n",
        "    agent2_QL = \n",
        "    agent2_FP = \n",
        "    agent2_Inf = \n",
        "    agent2_Mixed_AA = \n",
        "\n",
        "    # Arrange agents into lists with labels\n",
        "    # Why? This structure allows us to automate all model-vs-model matchups in nested loops.\n",
        "    agents1_dict_list = [\n",
        "        {'model': 'QL', 'agent': agent1_QL},\n",
        "        {'model': 'FP', 'agent': agent1_FP},\n",
        "        {'model': 'Inf', 'agent': agent1_Inf},\n",
        "        {'model': 'MIIL', 'agent': agent1_MIIL}\n",
        "    ]\n",
        "    agents2_dict_list = [\n",
        "        {'model': 'QL', 'agent': agent2_QL},\n",
        "        {'model': 'FP', 'agent': agent2_FP},\n",
        "        {'model': 'Inf', 'agent': agent2_Inf},\n",
        "        {'model': 'Mixed_AA', 'agent': agent2_Mixed_AA}\n",
        "    ]\n",
        "\n",
        "    # --- Run all possible pairings of agent1 vs agent2 ---\n",
        "    for entry1 in agents1_dict_list:\n",
        "        for entry2 in agents2_dict_list:\n",
        "            mod1 = entry1['model']\n",
        "            mod2 = entry2['model']\n",
        "            agent1 = entry1['agent']\n",
        "            agent2 = entry2['agent']\n",
        "\n",
        "            # Reset agents' histories and internal states before each match-up\n",
        "            # Why? Ensures each match starts fresh, unaffected by previous matches.\n",
        "            agent1.history = []\n",
        "            agent2.history = []\n",
        "            agent1.x = agent1.x_init\n",
        "            agent2.x = agent2.x_init\n",
        "\n",
        "            # Simulate the game: both agents interact and learn over n_trials\n",
        "            history = \n",
        "\n",
        "            # Store results for later analysis\n",
        "            # Including: which simulation, which models, both agent objects, the full game history, and total reward for agent1\n",
        "            full_sims_hist.append({\n",
        "                'sim_id': sim,\n",
        "                'model': mod1,\n",
        "                'other_agent': mod2,\n",
        "                'agent1': agent1,\n",
        "                'agent2': agent2,\n",
        "                'history': history,\n",
        "                'total_reward': sum([trial_data[\"agent1_reward\"] for trial_data in history])\n",
        "            })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "sO4wDiZUC1I5",
        "outputId": "09426b20-256a-40bd-becc-0cf652390b41"
      },
      "outputs": [],
      "source": [
        "# Plot the results of the simulations\n",
        "\n",
        "# Conversion to dataframes\n",
        "df_models = pd.DataFrame(full_sims_hist)\n",
        "\n",
        "# Violin plots\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.violinplot(x='other_agent', y='total_reward', hue='model', data=df_models, split=False, palette='Set2')\n",
        "\n",
        "# Style\n",
        "plt.title('Performance Across Opponents', fontsize=14)\n",
        "plt.ylabel('Total Reward')\n",
        "plt.xlabel('Other (Agent 2)')\n",
        "plt.legend(title='Me (Agent 1)', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKCNNrUiNLwQ"
      },
      "source": [
        "# Model Fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkY9p1XwYEDE"
      },
      "source": [
        "## Why Do We Fit Models?\n",
        "If we consider a set of data drawn from an unknown process, for example the consecutive choices of a participant in a game, the question that we want to address is “what is the process that generated these data?”. Computational models are propositions of candidate processes that could have generated such data, summed up into mathematical equations. To make it simple, you can see these mathematical equations, or computational models, as constituted by “bricks”. These bricks are more or less easily interpretable and they modulate the behaviour predicted by the model. Our aim is to identify the best fitting model to our data, because its core mechanisms, or bricks, are good candidate mechanisms that could explain the data generation process. And to do so, we have to find the best learning parameters ($\\theta$ and $\\phi$) of each model for our data.\n",
        "\n",
        "<img src=\"images\\Model_blocks.jpg\" alt=\"Models are built with 'blocks'\" width=\"1000\"/>\n",
        "\n",
        "In summary:\n",
        "- Computational models help explain behavior in social games.\n",
        "- Parameters (e.g., learning rates, belief depths) are latent and must be inferred.\n",
        "- **Model fitting** is the process of estimating these unobserved parameters from observed behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## General Approaches to Model Fitting\n",
        "\n",
        "Different approaches exist to fit models. The 3 main ones are:\n",
        "\n",
        "### Maximum Likelihood Estimation (MLE) – “Let the Data Speak”\n",
        "\n",
        "#### Core Idea:\n",
        "- Finds the model that makes the observed data most likely, without considering anything else (like prior beliefs).\n",
        "- Estimates parameters that **maximize the likelihood** of observed data, i.e. P(data | parameters).\n",
        "\n",
        "NB: most often, we work with the Log-Likelihood (LL) rather than the likelihood for (computational) practical reasons. \n",
        "\n",
        "#### Example:\n",
        "You flip a coin 10 times and get 7 heads. You want to estimate the probability of heads, say θ.\n",
        "\n",
        "MLE asks: “Which value of θ makes it most likely that you would get 7 heads out of 10?”\n",
        "\n",
        "Answer: θ = 0.7\n",
        "\n",
        "The estimate will be a single value, based purely on the data.\n",
        "\n",
        "### Maximum a Posteriori (MAP) – “Let the Data Speak, but Consider Your Beliefs Too”\n",
        "\n",
        "#### Core Idea:\n",
        "- Like MLE, but you also include prior beliefs about what values are likely, using Bayes’ Rule.\n",
        "- It incorporates **prior beliefs** about parameters.\n",
        "- MAP = argmax [ likelihood × prior ]\n",
        "\n",
        "Reminder of Bayes' Rule:\n",
        "$$\n",
        "posterior = \\frac{likelihood * prior}{evidence}\n",
        "$$\n",
        "\n",
        "This can be illustrated as follows:\n",
        "\n",
        "<img src=\"images\\Bayes_inference.jpg\" alt=\"Bayes inference rule\" width=\"550\"/>\n",
        "\n",
        "For the observed data $x$ and model parameters $\\theta$, this gives:\n",
        "$$\n",
        "p(\\theta|x) = \\frac{p(x|\\theta)*p(\\theta)}{p(x)}\n",
        "$$\n",
        "\n",
        "#### Example:\n",
        "With the same example as previously, suppose you believe the coin is fair, i.e., θ is probably close to 0.5, before seeing any flips.\n",
        "\n",
        "MAP combines:\n",
        "\n",
        "- Likelihood (what the data says)\n",
        "- Prior (what you believed before)\n",
        "\n",
        "MAP asks:\n",
        "\n",
        "“Given the data and my prior belief, which value of θ is most likely?”\n",
        "\n",
        "Depending on the strength of your prior beliefs, the answer might now be, for example: θ = 0.65 (slight prior belief of coin fairness, which slightly pulls θ toward 0.5 compared to previously with MLE).\n",
        "\n",
        "The estimate will be, again, a single value, based not only on the data but also on your prior beliefs.\n",
        "\n",
        "### Bayesian Inference – “Use the Whole Distribution”\n",
        "\n",
        "#### Core Idea:\n",
        "- Don’t just pick the most likely value—keep the entire distribution of possible values, to represent uncertainty.\n",
        "- Estimates a **posterior distribution** over parameters.\n",
        "- Supports uncertainty quantification and model comparison.\n",
        "\n",
        "#### Example:\n",
        "Instead of saying “θ = 0.7” or “θ = 0.65”, Bayesian inference says:\n",
        "“Given my data and prior, here’s the full posterior distribution over θ.”\n",
        "\n",
        "This gives you:\n",
        "- A most likely value (~max of the distribution)\n",
        "- A range of plausible values (~distribution values within a standard deviation centered around the max)\n",
        "- A sense of uncertainty (~standard deviation)\n",
        "\n",
        "The estimate will be a distribution, and not a single value anymore as in MLE and MAP.\n",
        "\n",
        "We will focus today on *Bayesian Inference*.\n",
        "\n",
        "### From Bayesian Inference to (Stochastic) Variational Inference\n",
        "- Exact inference is often intractable.\n",
        "- **Variational Inference (VI)** approximates the true posterior with a simpler distribution.\n",
        "- The goal is to optimize this distribution to be close to the real posterior.\n",
        "- Stochastic Variational Inference (SVI) uses stochastic gradient descent (e.g., Adam).\n",
        "- Optimizes the **Evidence Lower Bound (ELBO)** (see https://pyro.ai/examples/svi_part_i.html#ELBO for more details).\n",
        "- Implemented in libraries like **Pyro**, **NumPyro**, **TensorFlow Probability** (Python) or **VBA-toolbox** (Matlab).\n",
        "\n",
        "Let's now fit our models to some of our generated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGiR7vkbYG29"
      },
      "outputs": [],
      "source": [
        "# Initialize Pyro models and guides\n",
        "# Why? In probabilistic programming, a 'model' defines how data is generated (including randomness and hidden variables),\n",
        "# while a 'guide' is an approximation for inference (how we guess hidden variables from observed data).\n",
        "# Here, we pair each learning model with its corresponding guide for use in inference tasks (e.g., parameter fitting).\n",
        "pyro_models = {\n",
        "    \"QL\": (q_learning_pyro_model, guide_Qlearn),\n",
        "    \"FP\": (fictitious_learner_pyro_model, guide_FPlayer),\n",
        "    \"Inf\": (influence_learning_pyro_model, guide_influence_learning),\n",
        "    \"MIIL\": (MIIL_pyro_model, guide_MIIL)\n",
        "}\n",
        "\n",
        "# Collect unique model names for easy reference elsewhere in the code.\n",
        "# Why? This makes it simple to iterate over all available models (e.g., for running inference, simulations, or comparisons).\n",
        "all_models = set(pyro_models.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.4.a) Fill the following code cell to fit the 4 models defined in the dictionary \"pyro_models\" to your data.\n",
        "\n",
        "Qu.4.b) Try different values for n_steps, tolerance and patience. What happens? What are these parameters used for? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k =  # TODO:choose the index of the data in full_sims_hist that you want to fit \n",
        "\n",
        "# Load the results of the selected simulation into a DataFrame for easier access and plotting\n",
        "sim_hist = pd.DataFrame(full_sims_hist[k]['history'])\n",
        "\n",
        "# Prepare the observed data needed for model fitting (actions and rewards for agent 1)\n",
        "data = {\n",
        "    'agent1_action': sim_hist['agent1_action'],\n",
        "    'agent2_action': sim_hist['agent2_action'],\n",
        "    'agent1_reward': sim_hist['agent1_reward']\n",
        "}\n",
        "\n",
        "# Loop through each model you want to fit (as defined in pyro_models)\n",
        "for fitted_model_name, (model_fn, guide_fn) in pyro_models.items():\n",
        "    print((f\"Fitting model {fitted_model_name}\"))\n",
        "\n",
        "    # Prepare model-specific input arguments\n",
        "    # Why? Each model expects its own inputs: e.g., game matrices, player number, belief vector size.\n",
        "    if(fitted_model_name==\"MIIL\"):\n",
        "        in_dict = {'game1': payoff_matrix_CG, 'game2': payoff_matrix_HaS, 'player': 1, 'dim_x': 3}\n",
        "    elif(fitted_model_name==\"Inf\"):\n",
        "        in_dict = {'game': payoff_matrix_CG, 'player': 1, 'dim_x': 1}\n",
        "    else:\n",
        "        in_dict = {'dim_x': 2}\n",
        "\n",
        "    # --- Fit the model to the data using probabilistic inference ---\n",
        "    # This step estimates the model parameters that best explain the observed actions and rewards.\n",
        "    # TODO: fill the missing inputs for fit_model() and compute_goodness_of_fit(). Explore the file CIX_course_Fitting_ToM_functions.py to see how to use these functions\n",
        "    res = fit_model(, , , , n_steps=, tolerance=, verbose=True, patience=)\n",
        "    # --- Evaluate the fit: how well does the model capture the observed behavior? ---\n",
        "    goodness_of_fit = compute_goodness_of_fit()\n",
        "    \n",
        "    # --- Visualize the results ---\n",
        "    # Plot how well the model's predictions align with the actual (simulated) data\n",
        "    plot_fit_results(res, sim_hist, fitted_model_name, n_trials)\n",
        "\n",
        "    # --- Report summary metrics for this model fit ---\n",
        "    # Accuracy: percentage of actions the model predicts correctly\n",
        "    accuracy = goodness_of_fit['accuracy']\n",
        "    print(f\"Model {fitted_model_name} accuracy: {accuracy:.2f}\")\n",
        "\n",
        "    # Balanced accuracy: accounts for class imbalance between actions\n",
        "    balanced_accuracy = goodness_of_fit['balanced_accuracy']\n",
        "    print(f\"Model {fitted_model_name} balanced accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "    # Log likelihood: how probable the observed data is under the fitted model (higher is better)\n",
        "    log_likelihood = goodness_of_fit['log_likelihood']\n",
        "    print(f\"Model {fitted_model_name} log likelihood: {log_likelihood:.2f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.5.a) We have fitted different models to some of our simulated data. Which model should we choose to explain our data? Based on what criterion? \n",
        "\n",
        "Qu.5.b) Can you find some limits to using accuracy, balanced accuracy, and log-likelihood to compare different models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Comparison \n",
        "### Why Compare Models?\n",
        "- Multiple models may explain the same behavior.\n",
        "- **Model comparison** helps us decide which model best accounts for the data.\n",
        "\n",
        "### Key Metrics\n",
        "- **Log Likelihood** (LL)\n",
        "$$\n",
        "LL = ln(P(x|\\theta,\\phi))\n",
        "$$\n",
        "- **AIC** (Akaike Information Criterion) / **BIC** (Bayesian Information Criterion): penalized likelihood criteria\n",
        "$$\n",
        "AIC = -2*(LL - n_{params})\n",
        "$$\n",
        "\n",
        "$$\n",
        "BIC = -2*(LL - \\frac{n_{params}*ln(n_{trials})}{2})\n",
        "$$\n",
        "\n",
        "- **ELBO** (evidence lower bound): \n",
        "$$\n",
        "ELBO=(Reconstruction accuracy)−(Regularization penalty)\n",
        "$$\n",
        "Maximizing ELBO improves the fit to the data (see https://pyro.ai/examples/svi_part_i.html#ELBO).\n",
        "\n",
        "### Fixed vs. Random Effects for model comparison\n",
        "- Fixed-effect analysis (FFX): assumes that a single model best describes all subjects (although subjects might still differ between each other with different model parameters).\n",
        "\n",
        "<img src=\"images\\ffxbms.jpg\" alt=\"FFX BMS\" width=\"200\"/>\n",
        "\n",
        "The same **model $m$** generated the **data $y_k$** of each **subject $k$** (possibly with different **model parameters $\\theta$ and $\\phi$**).\n",
        "\n",
        "To do a FFX model comparison:\n",
        "1. Choose your (log-) model evidence, i.e. criterion of comparison (e.g. AIC, BIC, ELBO...).\n",
        "2. For each subject, fit each model and get the corresponding model evidence (AIC, BIC, ELBO...).\n",
        "3. Sum or average the evidences over subjects for each model.\n",
        "4. Select the lowest (or highest, depending on the chosen criterion) model evidence to select the best model for the group.\n",
        "\n",
        "- Random-effect analysis (RFX): assumes that your group is drawn from a population where each model is present in different proportions, with an unknown population distribution (described in terms of model frequencies/proportions)\n",
        "\n",
        "<img src=\"images\\rfxbms.jpg\" alt=\"RFX BMS\" width=\"200\"/>\n",
        "\n",
        "Your population has a **model frequency profile $r$** (repartition of the models in the population) from which each **subject $k$** has a given **model $m_k$** and generated the **data $y_k$**. \n",
        "\n",
        "To do a RFX model comparison:\n",
        "1. Choose your (log-) model evidence, i.e. criterion of comparison (e.g. AIC, BIC, ELBO...).\n",
        "2. For each subject, fit each model and get the corresponding model evidence (AIC, BIC, ELBO...).\n",
        "3. For each subject, select the lowest (or highest, depending on the chosen criterion) model evidence to select the best model among the tested models for the subject.\n",
        "4. Compute model frequencies (frequency at which each model was selected as the best model). You can also compute model exceedance probabilities (measures how likely it is that any given model is more frequent than all other models in the comparison set, see Stephan et al., Neuroimage 2009 for more details).\n",
        "\n",
        "In summary:\n",
        "- **Fixed effects (FFX)**: assumes one best model across all participants.\n",
        "- **Random effects (RFX)**: allows for individual variability.\n",
        "\n",
        "> Tools like `pyBMS`, `SPM`, or custom Dirichlet estimation can help implement RFX comparisons.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.6.a) Fill the following to fit multiple simulated agents with all 4 models.\n",
        "\n",
        "Qu.6.b) Compute the AIC and BIC for each fitted model and subject."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fit_result = []  # This list will collect the results of each model fit for all simulated subjects\n",
        "\n",
        "nb_sbj =  # TODO: Choose the number of simulated subjects to fit\n",
        "# Why? Fitting multiple agents allows us to check how well each model generalizes across different simulated behaviors.\n",
        "\n",
        "for \n",
        "    # For each simulated subject (agent history)...\n",
        "    sim_hist = pd.DataFrame(full_sims_hist[k]['history'])  # Extract the k-th simulated game as a DataFrame\n",
        "\n",
        "    # Prepare the observed data: what actions and rewards were seen for agent 1 in this simulation\n",
        "    data = {\n",
        "        'agent1_action': sim_hist['agent1_action'],\n",
        "        'agent2_action': sim_hist['agent2_action'],\n",
        "        'agent1_reward': sim_hist['agent1_reward']\n",
        "    }\n",
        "\n",
        "    # Fit every model in pyro_models to this simulated subject's data\n",
        "    for fitted_model_name, (model_fn, guide_fn) in pyro_models.items():\n",
        "        print((f\"Fitting model {fitted_model_name}\"))\n",
        "\n",
        "        # Prepare model-specific input arguments (game matrices, hidden state dimensions, etc.)\n",
        "        if(fitted_model_name==\"MIIL\"):\n",
        "            in_dict = {'game1': payoff_matrix_CG, 'game2': payoff_matrix_HaS, 'player': 1, 'dim_x': 3}\n",
        "        elif(fitted_model_name==\"Inf\"):\n",
        "            in_dict = {'game': payoff_matrix_CG, 'player': 1, 'dim_x': 1}\n",
        "        else:\n",
        "            in_dict = {'dim_x': 2}\n",
        "\n",
        "        # TODO: fill the missing inputs for fit_model() and compute_goodness_of_fit(). Explore the file CIX_course_Fitting_ToM_functions.py to see how to use these functions.\n",
        "        # Run the Pyro fitting process, which estimates parameters so the model explains the data as well as possible\n",
        "        res = fit_model( ,  ,  ,  ,  ,  , verbose=True,  )\n",
        "        # Compute how well the fitted model explains the data (accuracy, log-likelihood, etc.)        \n",
        "        goodness_of_fit = compute_goodness_of_fit( )\n",
        "\n",
        "        # Collect all relevant info for later analysis or comparison\n",
        "        fit_result.append({\n",
        "            'simulated_model': full_sims_hist[k]['model'],      # What model was used to generate this subject's data?\n",
        "            'fitted_model': fitted_model_name,                  # What model are we fitting?\n",
        "            'participant_nb': k,                                # Which simulated subject is this?\n",
        "            'model_fit_result': res,                            # Full fit result object (parameters, etc.)\n",
        "            'goodness_of_fit': goodness_of_fit                  # Fit metrics for this run\n",
        "        })  \n",
        "        \n",
        "        # Print summary metrics for this fit, so you can monitor performance and spot issues\n",
        "        # Model accuracy\n",
        "        accuracy = goodness_of_fit['accuracy']\n",
        "        print(f\"Model {fitted_model_name} accuracy: {accuracy:.2f}\")\n",
        "\n",
        "        # Model balanced accuracy\n",
        "        balanced_accuracy = goodness_of_fit['balanced_accuracy']\n",
        "        print(f\"Model {fitted_model_name} balanced accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "        # Model log likelihood\n",
        "        log_likelihood = goodness_of_fit['log_likelihood']\n",
        "        print(f\"Model {fitted_model_name} log likelihood: {log_likelihood:.2f}\")\n",
        "\n",
        "        #TODO: compute the AIC and BIC for the fitted model\n",
        "        # Model AIC\n",
        "        aic = \n",
        "        print(f\"Model {fitted_model_name} AIC: {aic:.2f}\")\n",
        "\n",
        "        # Model BIC\n",
        "        bic = \n",
        "        print(f\"Model {fitted_model_name} BIC: {bic:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.7 Do the FFX model comparison for different criteria (AIC, BIC, ELBO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sum (or average) model evidence across participants for each fitted model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.8.a) Do the RFX model comparison based on the ELBO criterion. You might need to use some functions in CIX_course_Fitting_ToM_functions.py. \n",
        "\n",
        "Qu.8.b) Plot model comparison results for the model frequencies and exceedance probabilities. You might need to use some functions in CIX_course_Fitting_ToM_functions.py. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert ELBOs to per-participant posteriors\n",
        "# Why? We need a matrix of model evidence (ELBOs) for each model/participant pair to perform random-effects Bayesian model comparison.\n",
        "# elbo_matrix: rows = participants, columns = models; each element is the ELBO (evidence lower bound) for that fit\n",
        "\n",
        "\n",
        "# Estimate RFX model frequencies using a Dirichlet distribution\n",
        "# Why? Random-effects (RFX) analysis estimates the probability (frequency) with which each model is best across a population, accounting for between-subject heterogeneity.\n",
        "# alphas: estimated Dirichlet parameters representing model frequencies across the group\n",
        "# xp: exceedance probabilities (probability that a given model is more frequent than any other)\n",
        "\n",
        "\n",
        "# Bootstrapping for confidence intervals on model frequency estimates\n",
        "# Why? Bootstrapping provides an empirical estimate of uncertainty (confidence intervals) around the model frequencies.\n",
        "\n",
        "\n",
        "# Report model frequencies and exceedance probabilities\n",
        "\n",
        "\n",
        "# Plot the results with confidence intervals\n",
        "# Why? Visualization helps communicate which models are most frequent and how certain we are about this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sanity checks: Model recovery and parameters recovery\n",
        "\n",
        "When you build a computational model (like to explain decision-making or behavior), it’s easy to get a good-looking fit to data—but does your model actually make sense?\n",
        "\n",
        "Two important sanity checks help answer that:\n",
        "- Parameter Recovery\n",
        "- Model Recovery\n",
        "\n",
        "## Parameter recovery – “Can I trust the numbers I’m estimating?”\n",
        "\n",
        "Suppose your model has parameters (e.g., learning rate, risk aversion, influence weight, etc.). You simulate data with known parameter values, then fit your model and check:\n",
        "\n",
        "Do the fitted parameters match the ones you used?\n",
        "\n",
        "### Why does it matter?\n",
        "If your model can't recover its own parameters reliably, any interpretations you make about “high learning rate” or “low risk aversion” are not trustworthy.\n",
        "\n",
        "### How does it work?\n",
        "- Pick a model and generate synthetic data using known parameters.\n",
        "- Fit the model to that data.\n",
        "- Compare the true vs recovered parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.9.a) If you plot true vs recovered parameters, what should it look like if the recovery is good? Plot what a good parameter recovery should look like with fake data points.\n",
        "\n",
        "Qu.9.b) Use your previously fitted data to do a parameter recovery for each parameter of different models. Start with the QL model. How is your recovery? \n",
        "\n",
        "Qu.9.c) What happens if you change n_steps, tolerance, patience then try parameter recovery again?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#What a good parameter recovery should look like:\n",
        "# Generate true parameter values\n",
        "\n",
        "\n",
        "# Generate \"recovered\" parameter values with small noise\n",
        "\n",
        "\n",
        "# Plot true vs recovered parameters\n",
        "\n",
        "\n",
        "#How to do it with our own simulated data:\n",
        "# Organize recovery data from results and full_sims_hist\n",
        "\n",
        "\n",
        "# Generate plots to visualize parameter recovery for each simulated model\n",
        "\n",
        "    # For each type of simulated model (e.g., QL, FP, Inf, MIIL)...\n",
        "\n",
        "    # Plot true vs recovered parameters\n",
        "    # Why? A good fit should have recovered values close to the true ones (points should fall along the identity line)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change the guide \"guide_fn\" in the fit_model() function to None. This will set it to a default guide that is automatically set and optimized. \n",
        "\n",
        "Qu.10. What happens to the fit? To the parameter recovery? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fit_result = []  # This list will store the outcome of fitting each model to each simulated subject\n",
        "\n",
        "nb_sbj = # TODO: choose the number of simulated subjects to fit and loop through them\n",
        "for \n",
        "    # For each simulated subject (agent run)...\n",
        "    sim_hist = pd.DataFrame(full_sims_hist[k]['history'])  \n",
        "    # Convert the simulation history to a DataFrame for easier manipulation\n",
        "    \n",
        "    # Prepare the data dictionary to be used in model fitting\n",
        "    # Why? This is the observed sequence of actions and rewards that your models will try to explain/predict.\n",
        "    data = {\n",
        "        'agent1_action': sim_hist['agent1_action'],\n",
        "        'agent2_action': sim_hist['agent2_action'],\n",
        "        'agent1_reward': sim_hist['agent1_reward']\n",
        "    }    \n",
        "\n",
        "    # Loop through each model you want to fit (as defined in pyro_models)\n",
        "    # Why? This allows you to systematically evaluate how well each model explains the observed behavior.\n",
        "    for fitted_model_name, (model_fn, guide_fn) in pyro_models.items():\n",
        "        print((f\"Fitting model {fitted_model_name}\"))\n",
        "\n",
        "        # Prepare model-specific arguments: different models need different inputs (e.g., number of hidden states, payoff matrices)\n",
        "        if(fitted_model_name==\"MIIL\"):\n",
        "            in_dict = {'game1': payoff_matrix_CG, 'game2': payoff_matrix_HaS, 'player': 1, 'dim_x': 3}\n",
        "        elif(fitted_model_name==\"Inf\"):\n",
        "            in_dict = {'game': payoff_matrix_CG, 'player': 1, 'dim_x': 1}\n",
        "        else:\n",
        "            in_dict = {'dim_x': 2}\n",
        "\n",
        "        # TODO: fill the missing inputs for fit_model() and compute_goodness_of_fit(). Explore the file CIX_course_Fitting_ToM_functions.py to see how to use these functions. Set guide to None.\n",
        "        res = fit_model( , , , , , , verbose=True, )\n",
        "        goodness_of_fit = compute_goodness_of_fit()\n",
        "\n",
        "        # Store all results for later analysis (model comparison, parameter recovery, etc.)\n",
        "        fit_result.append({\n",
        "            'simulated_model': full_sims_hist[k]['model'],\n",
        "            'fitted_model': fitted_model_name,\n",
        "            'participant_nb': k,\n",
        "            'model_fit_result': res,\n",
        "            'goodness_of_fit': goodness_of_fit\n",
        "        })    \n",
        "\n",
        "        # Model accuracy\n",
        "        accuracy = goodness_of_fit['accuracy']\n",
        "        print(f\"Model {fitted_model_name} accuracy: {accuracy:.2f}\")\n",
        "\n",
        "        # Model balanced accuracy\n",
        "        balanced_accuracy = goodness_of_fit['balanced_accuracy']\n",
        "        print(f\"Model {fitted_model_name} balanced accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "        # Model log likelihood\n",
        "        log_likelihood = goodness_of_fit['log_likelihood']\n",
        "        print(f\"Model {fitted_model_name} log likelihood: {log_likelihood:.2f}\")\n",
        "\n",
        "        # TODO: compute the AIC and BIC for the fitted model\n",
        "        # Model AIC\n",
        "        aic = \n",
        "        print(f\"Model {fitted_model_name} AIC: {aic:.2f}\")\n",
        "\n",
        "        # Model BIC\n",
        "        bic = \n",
        "        print(f\"Model {fitted_model_name} BIC: {bic:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Organize recovery data from results and full_sims_hist\n",
        "\n",
        "\n",
        "# Generate plots\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model recovery – “Can I tell models apart?”\n",
        "\n",
        "Suppose you have two or more competing models. You simulate data using one of them, and then try to fit all the models to this simulated data. If you’re doing it right, the correct model (the one that generated the data) should be the one you recover.\n",
        "\n",
        "### Why does it matter?\n",
        "If your model comparison can't identify the right model when you already know the answer, you shouldn't trust it on real data...\n",
        "\n",
        "### How does it work?\n",
        "- Pick several models.\n",
        "- Simulate behavior from each model using known parameters.\n",
        "- Fit all models to that simulated data.\n",
        "- See if you correctly identify the model that generated the data.\n",
        "- If not → data generated by your models is too similar, or the fitting procedure is flawed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To represent a model recovery, we usually plot the confusion matrix showing how often the right model is chosen. For each simulated model, we plot the estimated frequencies after fitting all models. \n",
        "\n",
        "Qu.11.a) If we have 4 models (e.g. the QL, FP, Influence and MIIL models), how would this matrix look like if the model recovery is good? Plot a dummy recovery matrix that would represent a good model recovery.\n",
        "\n",
        "Qu.11.b) Plot a dummy matrix that would represent a bad model recovery. What is wrong with your matrix? What would it mean in terms of your fitting procedure or models? \n",
        "\n",
        "Qu.12 Think about what you can possibly do to improve model recovery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up dummy 4x4 confusion matrix\n",
        "# Good recovery matrix\n",
        "\n",
        "\n",
        "# Bad recovery matrix \n",
        "\n",
        "\n",
        "# Model names\n",
        "\n",
        "\n",
        "# Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMCIZ3P0bIXf"
      },
      "source": [
        "Ideally, we would like to perform a model recovery on our simulated data. However, the fitting procedure will take a very long time. It would need parallelization. So this will be for another time.\n",
        "\n",
        "End of session."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
