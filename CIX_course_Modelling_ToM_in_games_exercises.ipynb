{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_XGVwCn-Qgn"
      },
      "source": [
        "<img src=\"images\\header_CIX2025.png\" alt=\"CIX 2025\" width=\"1200\"/>\n",
        "\n",
        "# Theory-of-Mind (ToM) models in games"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nBquLV1-hgF"
      },
      "source": [
        "Toan Nong (post-doc at CNRS, Marc Jeannerod Institute, Dreher lab, Lyon): tnong@isc.cnrs.fr \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## General introduction to ToM\n",
        "Trying to read minds, so wondering what and how other people think is what we call Theory-of-Mind (ToM). \n",
        "More precisely, it is defined as: “The capacity to represent that and how agents represent the world from their own points of view (meta-representation).” (Rakoczy, Nat. Rev. Psy. 2022).\n",
        "A simple representation would include only the basic characteristics of others, while meta-representation, so ToM, would incorporate the representation of others’ representation. \n",
        "\n",
        "<img src=\"images\\Basic_vs_meta_representation.jpg\" alt=\"Basic vs Meta representation\" width=\"1000\"/>\n",
        "\n",
        "To study ToM, we can think on 3 different levels, sometimes referred to as Marr’s framework. The first level to understand is the *goal* of the social behaviour that we want to study, here Theory of mind (or *why* ToM is used). This requires to understand the context in which we study ToM (competitive/cooperative/a mixed game? Dyadic/Fixed group/Dynamic network  interaction? Etc...) and its subcomponents (Beliefs? Intentions? Reasoning? Emotions? Etc...) that we want to study. The 2nd level asks what algorithms are applied when ToM is deployed in a given context and for specific subcomponents. This would require to specify the computational models of the different ToM subcomponents. The last implementational level asks how ToM subcomponents are physically encoded. This requires to study the brain networks, areas, cells, etc. that lead to the emergence of the 2 first levels of ToM. We will here focus on the algorithmic level.\n",
        "\n",
        "<img src=\"images\\ToM_Marrs_levels.jpg\" alt=\"ToM Marr's levels\" width=\"600\"/>\n",
        "\n",
        "More precisely, we will only focus on some specific ToM subcomponents used in strategic games that have been well studied on the 3 different Marr’s levels. To present the first one, imagine the following situation:\n",
        "\n",
        "A lazy worker and an inspector. The inspector’s aim is that the worker works without being behind his back all the time. If the worker slacks off too much, the inspector will come more often. If the worker works enough, the inspector will come less often. In this case, the worker’s action has an influence on the inspector’s action (and vice-versa). This exemple shows how representing the influence of one’s action on the other’s decision is useful. See also Hampton et al., PNAS 2008.\n",
        "\n",
        "As for the 2nd subcomponent that we will study, think about diplomatic relations between neighbouring countries. They must always be on the watch for the intentions of the other countries that can change suddenly. This would require the ability to adapt to non-signaled changes of intentions. Another example would be unknown agents in a network: these agents could pretend to have good intentions. But they might switch to malevolent actions suddenly. If they are caught and rehabilitated, they might go back to behave as \"good\" agents. But they might still be tempted to act badly in the future.\n",
        "\n",
        "<img src=\"images\\ToM_subcomponents_in_strategic_games.jpg\" alt=\"ToM subcomponents in strategic games\" width=\"800\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models of decisions in social strategic games\n",
        "\n",
        "The question now is: How can we model these ToM subcomponents? We will see mainly 4 decision models usable in strategic games: the Q-Learning (QL), Fictitious Play/Fictitious Learning (FP), Influence and Mixed-Intentions Influence Learning (MIIL) models.\n",
        "\n",
        "### Q-Learning\n",
        "\n",
        "Q-learning is a **model-free reinforcement learning algorithm**.  \n",
        "Agents update their estimates of action values based on **received rewards**.\n",
        "\n",
        "**Core Idea**:  \n",
        "The agent maintains a value function $Q(a)$ for each action $a$, updated as:\n",
        "\n",
        "$$\n",
        "Q(a_t) \\leftarrow Q(a_t) + \\alpha \\cdot (r_t - Q(a_t))\n",
        "$$\n",
        "\n",
        "- $ \\alpha $: learning rate (0 < α ≤ 1)  \n",
        "- $ r_t $: reward received at time $t$   \n",
        "- $ a_t $: action taken at time $t$ \n",
        "\n",
        "**Action selection** uses a softmax policy:\n",
        "\n",
        "$$\n",
        "P(a_t) = \\frac{e^{\\beta Q(a_t)}}{\\sum_{a'} e^{\\beta Q(a')}}\n",
        "$$\n",
        "\n",
        "- $\\beta$ : inverse temperature controlling exploration vs. exploitation\n",
        "\n",
        "### Fictitious Learning\n",
        "\n",
        "Fictitious learning is a **value-based learning approach** where agents update their estimates of **all actions**, not just the one chosen.  \n",
        "It blends real experience with **counterfactual (fictitious)** reasoning to estimate what would have happened if a different action had been taken.\n",
        "\n",
        "**Core Idea**:  \n",
        "The agent maintains a value function $Q(a)$ for each action $a$, and updates **both** chosen and unchosen actions:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Q(a_t) &\\leftarrow Q(a_t) + \\alpha \\cdot (r_t - Q(a_t)) \\quad &&\\text{(chosen action)} \\\\\\\\\n",
        "Q(a_{\\neg t}) &\\leftarrow Q(a_{\\neg t}) + \\alpha \\cdot (\\hat{r}_{\\neg t} - Q(a_{\\neg t})) \\quad &&\\text{(unchosen action)}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "- $\\alpha$: learning rate (0 < α ≤ 1)  \n",
        "- $r_t$: actual reward received for action $a_t$\n",
        "- $\\hat{r}_{\\neg t}$: **fictitious reward** for unchosen action (e.g., expected or modeled)\n",
        "\n",
        "**Action selection** uses a softmax policy:\n",
        "\n",
        "$$\n",
        "P(a_t) = \\frac{e^{\\beta Q(a_t)}}{\\sum_{a'} e^{\\beta Q(a')}}\n",
        "$$\n",
        "\n",
        "- $\\beta$: inverse temperature controlling exploration vs. exploitation  \n",
        "- Higher $\\beta$ makes the policy more deterministic (greedy), while lower $\\beta$ increases exploration.\n",
        "\n",
        "Fictitious learning is commonly applied in **model-based reinforcement learning**, **cognitive modeling**, and **social learning**, where agents reason beyond direct experience.\n",
        "\n",
        "### Influence Learning Model\n",
        "\n",
        "Influence learning adds a **recursive social component** (ToM).\n",
        "\n",
        "**Core Idea**:  \n",
        "The agent believes the opponent is learning from them. It adjusts its own action values based on **how it thinks its own past actions influenced the opponent**.\n",
        "\n",
        "It introduces a second-order update via an **influence term** $I_t(a)$:\n",
        "\n",
        "$$\n",
        "Q(a_t) \\leftarrow Q(a_t) + \\eta \\cdot (r_t - Q(a_t)) + \\lambda \\cdot I_t(a_t)\n",
        "$$\n",
        "\n",
        "- $\\eta$: self learning rate  \n",
        "- $\\lambda$: influence sensitivity  \n",
        "- $I_t(a)$: estimated influence of agent's action on opponent's future behavior\n",
        "\n",
        "Still uses softmax to pick actions:\n",
        "\n",
        "$$\n",
        "P(a_t) = \\frac{e^{\\beta Q(a_t)}}{\\sum_{a'} e^{\\beta Q(a')}}\n",
        "$$\n",
        "\n",
        "\n",
        "### Mixed-Intentions Influence Learning (MIIL) Model\n",
        "\n",
        "The **MIIL model** (Philippe et al., 2024, *Nature Communications*) is a computational model designed for agents interacting across **two types of games** (e.g., coordination and competition). It captures a mix of **belief learning** and **influence learning**, under uncertainty about the opponent's intentions.\n",
        "\n",
        "---\n",
        "\n",
        "#### Hidden States\n",
        "\n",
        "MIIL uses **three internal beliefs**, represented in log-odds (inverse sigmoid) space:\n",
        "\n",
        "- $x_0$: belief about opponent's behavior in Game 1\n",
        "- $x_1$: belief about opponent's behavior in Game 2\n",
        "- $x_2$: belief about **which game** is being played\n",
        "\n",
        "These are updated based on prediction errors at two levels.\n",
        "\n",
        "---\n",
        "\n",
        "#### Update Equations\n",
        "\n",
        "The MIIL update rule combines first-order belief learning and second-order influence learning.\n",
        "\n",
        "Let:\n",
        "- $P(o=1 \\mid \\text{Game 1}) = \\sigma(x_0)$\n",
        "- $P(o=1 \\mid \\text{Game 2}) = \\sigma(x_1)$\n",
        "- $P(\\text{Game 1}) = \\sigma(x_2)$\n",
        "\n",
        "The agent observes:\n",
        "- Opponent’s action $o$\n",
        "- Its own action $a$\n",
        "\n",
        "**Prediction errors**:\n",
        "- First-order PE: discrepancy between $o$ and prior beliefs\n",
        "- Second-order PE: discrepancy between agent's own action and what it believes the opponent expects from them\n",
        "\n",
        "These are combined using learning rates $\\eta$ (belief update) and $\\lambda$ (influence learning).\n",
        "\n",
        "The update uses:\n",
        "$$\n",
        "p_\\text{game1}' = p_\\text{game1} + \\eta \\cdot PE_1^\\text{game1} + \\lambda \\cdot \\text{InfluenceTerm}_1\n",
        "$$\n",
        "and similarly for game 2. The belief about the game is updated via a softmax-like transformation of the two internal beliefs.\n",
        "\n",
        "---\n",
        "\n",
        "#### Decision Rule\n",
        "\n",
        "The agent chooses its action probabilistically based on a **mixture of decision values** from both games:\n",
        "\n",
        "$$\n",
        "P(a = 1) = \\sigma\\left( P_\\text{game} \\cdot DV_\\text{game1} + (1 - P_\\text{game}) \\cdot DV_\\text{game2} \\right)\n",
        "$$\n",
        "\n",
        "Where $DV$ (decision value) for each game depends on the expected reward given the belief about the opponent.\n",
        "\n",
        "---\n",
        "\n",
        "#### Parameters\n",
        "\n",
        "The MIIL model includes:\n",
        "- $\\eta$: belief learning rate\n",
        "- $\\lambda$: influence learning rate\n",
        "- $\\beta$: precision of the softmax choice\n",
        "- slope and bias: for the belief about game identity\n",
        "\n",
        "---\n",
        "\n",
        "#### Key Features\n",
        "\n",
        "- Tracks **multiple intentions** across different game contexts.\n",
        "- Updates beliefs about **how the opponent reacts to one's own actions**.\n",
        "- Learns which type of game is likely being played.\n",
        "\n",
        "---\n",
        "\n",
        "In summary:\n",
        "\n",
        "<img src=\"images\\Models_general_presentation.jpg\" alt=\"ToM models general description\" width=\"1200\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rai1JqCl92hS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from scipy.special import digamma\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "from collections import defaultdict\n",
        "from scipy.stats import beta as beta_dist, gamma as gamma_dist\n",
        "\n",
        "# Importing the custom module for ToM functions\n",
        "from CIX_course_Modelling_ToM_functions import *\n",
        "from CIX_course_Fitting_ToM_functions import *\n",
        "from CIX_course_Sanity_checks_functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-EgkqUxQAMP"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility (optional)\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "pyro.set_rng_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWyU6PEzBOtA"
      },
      "source": [
        "## Simulation of decision agents with or without ToM subcomponents\n",
        "\n",
        "### Models structure\n",
        "The models have been implemented in CIX_course_Modelling_ToM_functions.py. The models are structured into an update (or evolution) function $f(u_t,\\theta)$ and a decision fonction $g(x_t,\\phi)$. As its names suggests, the update function updates the hidden states $x_t$ of the model and will need as inputs: the data $u_t$, the learning parameter(s) of the evolution function $\\theta$. As for the decision function, it returns the decision of the model $y_t$ based on the hidden states $x_t$ and the learning parameter(s) of the decision function $\\phi$. These functions sometimes also take hyperparameters as inputs, i.e. parameters that can configure part of the model's learning process, but we will not cover this today.\n",
        "\n",
        "<img src=\"images\\Decision_Model_structure.png\" alt=\"Models strucure\" width=\"1000\"/>\n",
        "\n",
        "You won't need to modify the following coding block. It is the main simulation loop function, and the initialization of the initial hidden states of a specific Mixed Artificial Agent (Mixed_AA), not to be mistaken with a MIIL agent. This agent switches between competitive and cooperative modes with a specific decision process described in Philippe et al. 2024 (see also f_Mixed_AA and g_Mixed_AA). Yo make it simple, it computes the probability that the other player chooses an action based on the outcomes and choices from the 2 previous trials, and either tries to coordinate (cooperative mode), or not to coordinate (competitive mode). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfLfb7FrBREu"
      },
      "outputs": [],
      "source": [
        "# /!\\ Don't modify this cell\n",
        "def simulate_agents(agent1, agent2, n_trials=260):\n",
        "    history = []\n",
        "    for t in range(n_trials):\n",
        "        a1 = agent1.choose_action(t)\n",
        "        a2 = agent2.choose_action(t)\n",
        "\n",
        "        agent1.update(other_action=a2, own_action=a1, t=t)\n",
        "        agent2.update(other_action=a1, own_action=a2, t=t)\n",
        "\n",
        "        # Save history\n",
        "        history.append({\n",
        "            \"trial_nb\": t + 1,\n",
        "            \"agent1_action\": a1,\n",
        "            \"agent2_action\": a2,\n",
        "            \"agent1_reward\": agent1.history[-1][2],\n",
        "            \"agent2_reward\": agent2.history[-1][2]\n",
        "        })\n",
        "    return history\n",
        "\n",
        "x_init_AA = {(\"chose_0\",\"0000\"): 4,(\"chose_0\",\"0001\"): 4,(\"chose_0\",\"0010\"): 4,(\"chose_0\",\"0011\"): 4,\n",
        "            (\"chose_0\",\"0100\"): 4,(\"chose_0\",\"0101\"): 4,(\"chose_0\",\"0110\"): 4,(\"chose_0\",\"0111\"): 4,\n",
        "            (\"chose_0\",\"1000\"): 4,(\"chose_0\",\"1001\"): 4,(\"chose_0\",\"1010\"): 4,(\"chose_0\",\"1011\"): 4,\n",
        "            (\"chose_0\",\"1100\"): 4,(\"chose_0\",\"1101\"): 4,(\"chose_0\",\"1110\"): 4,(\"chose_0\",\"1111\"): 4,\n",
        "            (\"occ\",\"0000\"): 8,(\"occ\",\"0001\"): 8,(\"occ\",\"0010\"): 8,(\"occ\",\"0011\"): 8,\n",
        "            (\"occ\",\"0100\"): 8,(\"occ\",\"0101\"): 8,(\"occ\",\"0110\"): 8,(\"occ\",\"0111\"): 8,\n",
        "            (\"occ\",\"1000\"): 8,(\"occ\",\"1001\"): 8,(\"occ\",\"1010\"): 8,(\"occ\",\"1011\"): 8,\n",
        "            (\"occ\",\"1100\"): 8,(\"occ\",\"1101\"): 8,(\"occ\",\"1110\"): 8,(\"occ\",\"1111\"): 8}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e26_hvgYBfUw"
      },
      "source": [
        "### Context\n",
        "We will simulate a dyadic interaction between 2 players that can choose between 2 actions. The agents will be rewarded depending on the combination of the 2 players' actions. This will be summed up in a 2x2x2 payoff matrix. We will first consider simple games: the Hide-and-Seek (HaS) and the Coordination Game (CG). In the HaS, the Hider has to hide in one of two locations to avoid the Seeker, while the Seeker has to guess where the Hider is. In the CG, players have to coordinate on the same action to be rewarded. The HaS is a competitive game while the CG is a cooperative game. We will consider situations where the game is fixed, but also situations where the game can (unexpectedly) change. First, we will initialize some simulations variables that will be needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMVrjHdHBijx"
      },
      "outputs": [],
      "source": [
        "# 2x2x2 payoff tables: [agent_action, opponent_action, player_role]\n",
        "# Example: Payoff[0,1,0] = payoff for player 1 when they pick 0 and other picks 1\n",
        "payoff_matrix_HaS = np.zeros((2,2,2))\n",
        "payoff_matrix_HaS[:,:,0] = [[1, 0], [0, 1]]  # Payoff for Player 1\n",
        "payoff_matrix_HaS[:,:,1] = [[0, 1], [1, 0]]  # Payoff for Player 2 (opposite)\n",
        "\n",
        "payoff_matrix_CG = np.zeros((2,2,2))\n",
        "payoff_matrix_CG[:,:,0] = [[1, 0], [0, 1]]  # Payoff for Player 1\n",
        "payoff_matrix_CG[:,:,1] = [[1, 0], [0, 1]]  # Payoff for Player 2\n",
        "\n",
        "n_trials = 100  # or whatever total number of trials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's first simulate 2 QL agents playing with each other at CG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example initialization for Q-learning agents\n",
        "x_init_q1 = np.random.randn(2)      # Q-values\n",
        "phi_q1 = np.random.randn(2)         # [log beta, bias]\n",
        "theta_q1 = np.random.randn(1)       # invsigmoid(alpha)\n",
        "\n",
        "x_init_q2 = np.random.randn(2)      # Q-values\n",
        "phi_q2 = np.random.randn(2)         # [log beta, bias]\n",
        "theta_q2 = np.random.randn(1)       # invsigmoid(alpha)\n",
        "\n",
        "agent1_QL = Agent(f_func=f_Qlearning, g_func=g_Qlearning, x_init=x_init_q1, phi=phi_q1, theta=theta_q1, game=payoff_matrix_CG, player_id=1)\n",
        "agent2_QL = Agent(f_func=f_Qlearning, g_func=g_Qlearning, x_init=x_init_q2, phi=phi_q2, theta=theta_q2, game=payoff_matrix_CG, player_id=2)\n",
        "\n",
        "# Simulate agents\n",
        "history_QL = simulate_agents(agent1_QL, agent2_QL, n_trials=n_trials)\n",
        "\n",
        "# Convert history to DataFrame for easier manipulation\n",
        "history_df_QL = pd.DataFrame(history_QL)\n",
        "\n",
        "# Plot agents actions on the same graph\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(25, 6))\n",
        "sns.pointplot(data=history_df_QL, x=\"trial_nb\", y=\"agent1_action\", label=\"Agent 1 Action\", color=\"blue\")\n",
        "sns.pointplot(data=history_df_QL, x=\"trial_nb\", y=\"agent2_action\", label=\"Agent 2 Action\", color=\"orange\")\n",
        "plt.title(\"Actions of Q-learning Agents Over Trials (CG)\")\n",
        "plt.xlabel(\"Trial Number\")\n",
        "plt.ylabel(\"Action (0 or 1)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot agents rewards on the same graph\n",
        "plt.figure(figsize=(25, 6))\n",
        "sns.pointplot(data=history_df_QL, x=\"trial_nb\", y=\"agent1_reward\", label=\"Agent 1 Reward\", color=\"blue\")\n",
        "sns.pointplot(data=history_df_QL, x=\"trial_nb\", y=\"agent2_reward\", label=\"Agent 2 Reward\", color=\"orange\")\n",
        "plt.title(\"Rewards of Q-learning Agents Over Trials (CG)\")\n",
        "plt.xlabel(\"Trial Number\")  \n",
        "plt.ylabel(\"Reward\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.1.a) Now do the same for the HaS.\n",
        "\n",
        "Qu.1.b) Simulate games for different agents in the HaS and CG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's make a QL agent play with a Mixed AA that switches between competitive and cooperative modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a boolean array for game trials for the Mixed Agents that will switch between games\n",
        "# Here, we assume that each block consists of block_size (e.g. 26) trials, with the first game1_size_in_blocks (e.g. 15) trials for Game 1 and the next block_size-game1_size_in_blocks for Game 2.\n",
        "block_size = 20  # Size of each block\n",
        "game1_size_in_blocks = 12  # Number of trials for Game 1 in each block\n",
        "game1_trials = np.zeros(n_trials, dtype=int)\n",
        "for start in range(0, n_trials, block_size):\n",
        "    # Game 1: trials 1–game1_size_in_blocks (0-based: start to start+game1_size_in_blocks)\n",
        "    game1_trials[start:start + game1_size_in_blocks] = 1\n",
        "game2_trials = 1-game1_trials  # Game 2 is the opposite of Game 1\n",
        "print(\"Game 1 trials:\", game1_trials)\n",
        "print(\"Game 2 trials:\", game2_trials)\n",
        "\n",
        "# Example initialization for Q-learning agents\n",
        "x_init_q1 = np.random.randn(2)      # Q-values\n",
        "phi_q1 = np.random.randn(2)         # [log beta, bias]\n",
        "theta_q1 = np.random.randn(1)       # invsigmoid(alpha)\n",
        "\n",
        "agent1_QL = Agent(f_func=f_Qlearning, g_func=g_Qlearning, x_init=x_init_q1, phi=phi_q1, theta=theta_q1, game=payoff_matrix_CG, player_id=1)\n",
        "agent2_Mixed_AA = MixedAgent(f_func=f_Mixed_AA, g_func=g_Mixed_AA, x_init=x_init_AA, phi=None, theta=None,\n",
        "                        game1=payoff_matrix_CG, game2=payoff_matrix_HaS, game1_trials=game1_trials, game2_trials=game2_trials, player_id=2)\n",
        "\n",
        "# Simulate agents\n",
        "history_QL_vs_Mixed_AA = simulate_agents(agent1_QL, agent2_Mixed_AA, n_trials=n_trials)\n",
        "\n",
        "# Convert history to DataFrame for easier manipulation\n",
        "history_df_QL_vs_Mixed_AA = pd.DataFrame(history_QL_vs_Mixed_AA)\n",
        "\n",
        "# Plot agents actions on the same graph\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(25, 6))\n",
        "sns.pointplot(data=history_df_QL_vs_Mixed_AA, x=\"trial_nb\", y=\"agent1_action\", label=\"Agent 1 Action\", color=\"blue\")\n",
        "sns.pointplot(data=history_df_QL_vs_Mixed_AA, x=\"trial_nb\", y=\"agent2_action\", label=\"Agent 2 Action\", color=\"orange\")\n",
        "plt.title(\"Actions of QL (Agent1) and Mixed AA (Agent2) Over Trials\")\n",
        "plt.xlabel(\"Trial Number\")\n",
        "plt.ylabel(\"Action (0 or 1)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot agents rewards on the same graph\n",
        "plt.figure(figsize=(25, 6))\n",
        "sns.pointplot(data=history_df_QL_vs_Mixed_AA, x=\"trial_nb\", y=\"agent1_reward\", label=\"Agent 1 Reward\", color=\"blue\")\n",
        "sns.pointplot(data=history_df_QL_vs_Mixed_AA, x=\"trial_nb\", y=\"agent2_reward\", label=\"Agent 2 Reward\", color=\"orange\")\n",
        "plt.title(\"Rewards of  QL (Agent1) and Mixed AA (Agent2) Over Trials\")\n",
        "plt.xlabel(\"Trial Number\")  \n",
        "plt.ylabel(\"Reward\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.2.a) What can you observe for the rewards of the different agents? \n",
        "\n",
        "Qu.2.b) Simulate a game between a MIIL agent vs a Mixed AA.\n",
        "\n",
        "Qu.2.c) What can you observe for the rewards of the different agents?\n",
        "\n",
        "Qu.2.d) What happens if you simulate longer games with more trials? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8jddsc0CHVB"
      },
      "source": [
        "Let's now run multiple game simulations and look at the average reward for each model against any other given model. We want to see if there is an advantage for any model against the others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_sim = 100\n",
        "n_trials = 300  # Total number of trials for each simulation\n",
        "\n",
        "# Create a boolean array for game trials for the Mixed Agents that will switch between games\n",
        "# Here, we assume that each block consists of block_size (e.g. 26) trials, with the first game1_size_in_blocks (e.g. 15) trials for Game 1 and the next block_size-game1_size_in_blocks for Game 2.\n",
        "block_size = 20  # Size of each block\n",
        "game1_size_in_blocks = 12  # Number of trials for Game 1 in each block\n",
        "game1_trials = np.zeros(n_trials, dtype=int)\n",
        "for start in range(0, n_trials, block_size):\n",
        "    # Game 1: trials 1–game1_size_in_blocks (0-based: start to start+game1_size_in_blocks)\n",
        "    game1_trials[start:start + game1_size_in_blocks] = 1\n",
        "game2_trials = 1-game1_trials  # Game 2 is the opposite of Game 1\n",
        "print(\"Game 1 trials:\", game1_trials)\n",
        "print(\"Game 2 trials:\", game2_trials)\n",
        "\n",
        "full_sims_hist = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.3.a) Fill the following block of code to run nb_sim simulations of different agents playing against each others.\n",
        "\n",
        "Qu.3.b) Is there any advantage of using one model vs another? Is ToM always useful? How come? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v9cpwj4CMIg"
      },
      "outputs": [],
      "source": [
        "# Loop through the number of simulations\n",
        "for \n",
        "    # Example for Q-learning agent\n",
        "    x_init_q =      \n",
        "    phi_q =       \n",
        "    theta_q =     \n",
        "\n",
        "    # Example for Fictitious learner\n",
        "    x_init_FP = \n",
        "    phi_FP = \n",
        "    theta_FP = \n",
        "\n",
        "    # Example for Influence learner\n",
        "    x_init_inf = \n",
        "    phi_inf = \n",
        "    theta_inf = \n",
        "\n",
        "    # Example for MIIL (Mixed-Intentions Influence Learner)\n",
        "    x_init_miil = \n",
        "    phi_miil = \n",
        "    theta_miil = \n",
        "\n",
        "    # Initialize agents\n",
        "    agent1_QL = \n",
        "    agent1_FP = \n",
        "    agent1_Inf = \n",
        "    agent1_MIIL = \n",
        "\n",
        "    agent2_QL = \n",
        "    agent2_FP = \n",
        "    agent2_Inf = \n",
        "    agent2_Mixed_AA = \n",
        "\n",
        "    agents1_dict_list = [{'model': 'QL','agent':agent1_QL}, {'model': 'FP','agent':agent1_FP}, {'model': 'Inf','agent':agent1_Inf}, {'model': 'MIIL','agent':agent1_MIIL}]\n",
        "    agents2_dict_list = [{'model': 'QL','agent':agent2_QL}, {'model': 'FP','agent':agent2_FP}, {'model': 'Inf','agent':agent2_Inf}, {'model': 'Mixed_AA','agent':agent2_Mixed_AA}]\n",
        "    # Run simulation\n",
        "    for entry1 in agents1_dict_list:\n",
        "        for entry2 in agents2_dict_list:\n",
        "            mod1 = entry1['model']\n",
        "            mod2 = entry2['model']\n",
        "            agent1 = entry1['agent']\n",
        "            agent2 = entry2['agent']\n",
        "\n",
        "            # Reset agents\n",
        "            agent1.history = []\n",
        "            agent2.history = []\n",
        "            agent1.x = agent1.x_init\n",
        "            agent2.x = agent2.x_init\n",
        "\n",
        "            # Simulate the agents\n",
        "            history = \n",
        "\n",
        "            # Save simulation history\n",
        "            full_sims_hist.append({\n",
        "                'sim_id': sim,\n",
        "                'model': mod1,\n",
        "                'other_agent': mod2,\n",
        "                'agent1': agent1,\n",
        "                'agent2': agent2,\n",
        "                'history': history,\n",
        "                'total_reward': sum([trial_data[\"agent1_reward\"] for trial_data in history])\n",
        "            })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "sO4wDiZUC1I5",
        "outputId": "09426b20-256a-40bd-becc-0cf652390b41"
      },
      "outputs": [],
      "source": [
        "# Plot the results of the simulations\n",
        "\n",
        "# Conversion to dataframes\n",
        "df_models = pd.DataFrame(full_sims_hist)\n",
        "\n",
        "# Violin plots\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.violinplot(x='other_agent', y='total_reward', hue='model', data=df_models, split=False, palette='Set2')\n",
        "\n",
        "# Style\n",
        "plt.title('Performance Across Opponents', fontsize=14)\n",
        "plt.ylabel('Total Reward')\n",
        "plt.xlabel('Other (Agent 2)')\n",
        "plt.legend(title='Me (Agent 1)', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKCNNrUiNLwQ"
      },
      "source": [
        "# Model Fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkY9p1XwYEDE"
      },
      "source": [
        "## Why Do We Fit Models?\n",
        "If we consider a set of data drawn from an unknown process, for example the consecutive choices of a participant in a game, the question that we want to address is “what is the process that generated these data?”. Computational models are propositions of candidate processes that could have generated such data, summed up into mathematical equations. To make it simple, you can see these mathematical equations, or computational models, as constituted by “bricks”. These bricks are more or less easily interpretable and they modulate the behaviour predicted by the model. Our aim is to identify the best fitting model to our data, because its core mechanisms, or bricks, are good candidate mechanisms that could explain the data generation process. And to do so, we have to find the best learning parameters ($\\theta$ and $\\phi$) of each model for our data.\n",
        "\n",
        "<img src=\"images\\Model_blocks.jpg\" alt=\"Models are built with 'blocks'\" width=\"1000\"/>\n",
        "\n",
        "In summary:\n",
        "- Computational models help explain behavior in social games.\n",
        "- Parameters (e.g., learning rates, belief depths) are latent and must be inferred.\n",
        "- **Model fitting** is the process of estimating these unobserved parameters from observed behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## General Approaches to Model Fitting\n",
        "\n",
        "Different approaches exist to fit models. The 3 main ones are:\n",
        "\n",
        "### Maximum Likelihood Estimation (MLE) – “Let the Data Speak”\n",
        "\n",
        "#### Core Idea:\n",
        "- Finds the model that makes the observed data most likely, without considering anything else (like prior beliefs).\n",
        "- Estimates parameters that **maximize the likelihood** of observed data, i.e. P(data | parameters).\n",
        "\n",
        "NB: most often, we work with the Log-Likelihood (LL) rather than the likelihood for (computational) practical reasons. \n",
        "\n",
        "#### Example:\n",
        "You flip a coin 10 times and get 7 heads. You want to estimate the probability of heads, say θ.\n",
        "\n",
        "MLE asks: “Which value of θ makes it most likely that you would get 7 heads out of 10?”\n",
        "\n",
        "Answer: θ = 0.7\n",
        "\n",
        "The estimate will be a single value, based purely on the data.\n",
        "\n",
        "### Maximum a Posteriori (MAP) – “Let the Data Speak, but Consider Your Beliefs Too”\n",
        "\n",
        "#### Core Idea:\n",
        "- Like MLE, but you also include prior beliefs about what values are likely, using Bayes’ Rule.\n",
        "- It incorporates **prior beliefs** about parameters.\n",
        "- MAP = argmax [ likelihood × prior ]\n",
        "\n",
        "Reminder of Bayes' Rule:\n",
        "$$\n",
        "posterior = \\frac{likelihood * prior}{evidence}\n",
        "$$\n",
        "\n",
        "This can be illustrated as follows:\n",
        "\n",
        "<img src=\"images\\Bayes_inference.jpg\" alt=\"Bayes inference rule\" width=\"550\"/>\n",
        "\n",
        "For the observed data $x$ and model parameters $\\theta$, this gives:\n",
        "$$\n",
        "p(\\theta|x) = \\frac{p(x|\\theta)*p(\\theta)}{p(x)}\n",
        "$$\n",
        "\n",
        "#### Example:\n",
        "With the same example as previously, suppose you believe the coin is fair, i.e., θ is probably close to 0.5, before seeing any flips.\n",
        "\n",
        "MAP combines:\n",
        "\n",
        "- Likelihood (what the data says)\n",
        "- Prior (what you believed before)\n",
        "\n",
        "MAP asks:\n",
        "\n",
        "“Given the data and my prior belief, which value of θ is most likely?”\n",
        "\n",
        "Depending on the strength of your prior beliefs, the answer might now be, for example: θ = 0.65 (slight prior belief of coin fairness, which slightly pulls θ toward 0.5 compared to previously with MLE).\n",
        "\n",
        "The estimate will be, again, a single value, based not only on the data but also on your prior beliefs.\n",
        "\n",
        "### Bayesian Inference – “Use the Whole Distribution”\n",
        "\n",
        "#### Core Idea:\n",
        "- Don’t just pick the most likely value—keep the entire distribution of possible values, to represent uncertainty.\n",
        "- Estimates a **posterior distribution** over parameters.\n",
        "- Supports uncertainty quantification and model comparison.\n",
        "\n",
        "#### Example:\n",
        "Instead of saying “θ = 0.7” or “θ = 0.65”, Bayesian inference says:\n",
        "“Given my data and prior, here’s the full posterior distribution over θ.”\n",
        "\n",
        "This gives you:\n",
        "- A most likely value (~max of the distribution)\n",
        "- A range of plausible values (~distribution values within a standard deviation centered around the max)\n",
        "- A sense of uncertainty (~standard deviation)\n",
        "\n",
        "The estimate will be a distribution, and not a single value anymore as in MLE and MAP.\n",
        "\n",
        "We will focus today on *Bayesian Inference*.\n",
        "\n",
        "### From Bayesian Inference to (Stochastic) Variational Inference\n",
        "- Exact inference is often intractable.\n",
        "- **Variational Inference (VI)** approximates the true posterior with a simpler distribution.\n",
        "- The goal is to optimize this distribution to be close to the real posterior.\n",
        "- Stochastic Variational Inference (SVI) uses stochastic gradient descent (e.g., Adam).\n",
        "- Optimizes the **Evidence Lower Bound (ELBO)** (see https://pyro.ai/examples/svi_part_i.html#ELBO for more details).\n",
        "- Implemented in libraries like **Pyro**, **NumPyro**, **TensorFlow Probability** (Python) or **VBA-toolbox** (Matlab).\n",
        "\n",
        "Let's now fit our models to some of our generated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGiR7vkbYG29"
      },
      "outputs": [],
      "source": [
        "# Initialize Pyro models and guides\n",
        "pyro_models = {\n",
        "    \"QL\": (q_learning_pyro_model, guide_Qlearn),\n",
        "    \"FP\": (fictitious_learner_pyro_model, guide_FPlayer),\n",
        "    \"Inf\": (influence_learning_pyro_model, guide_influence_learning),\n",
        "    \"MIIL\": (MIIL_pyro_model, guide_MIIL)\n",
        "}\n",
        "\n",
        "# Collect unique models\n",
        "all_models = set(pyro_models.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.4.a) Fill the following code cell to fit the 4 models defined in the dictionary \"pyro_models\" to your data.\n",
        "\n",
        "Qu.4.b) Try different values for n_steps, tolerance and patience. What happens? What are these parameters used for? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k =  # TODO:choose the index of the data in full_sims_hist that you want to fit \n",
        "sim_hist = pd.DataFrame(full_sims_hist[k]['history'])\n",
        "data = {'agent1_action': sim_hist['agent1_action'], 'agent2_action': sim_hist['agent2_action'], 'agent1_reward': sim_hist['agent1_reward']}\n",
        "for fitted_model_name, (model_fn, guide_fn) in pyro_models.items():\n",
        "    print((f\"Fitting model {fitted_model_name}\"))\n",
        "\n",
        "    if(fitted_model_name==\"MIIL\"):\n",
        "        in_dict = {'game1': payoff_matrix_CG, 'game2': payoff_matrix_HaS, 'player': 1, 'dim_x': 3}\n",
        "    elif(fitted_model_name==\"Inf\"):\n",
        "        in_dict = {'game': payoff_matrix_CG, 'player': 1, 'dim_x': 1}\n",
        "    else:\n",
        "        in_dict = {'dim_x': 2}\n",
        "\n",
        "    # TODO: fill the missing inputs for fit_model() and compute_goodness_of_fit(). Explore the file CIX_course_Fitting_ToM_functions.py to see how to use these functions\n",
        "    res = fit_model(, , , , n_steps=, tolerance=, verbose=True, patience=)\n",
        "    goodness_of_fit = compute_goodness_of_fit()\n",
        "    \n",
        "    # Plot the results of the fitting\n",
        "    plot_fit_results(res, sim_hist, fitted_model_name, n_trials)\n",
        "\n",
        "    # Model accuracy\n",
        "    accuracy = goodness_of_fit['accuracy']\n",
        "    print(f\"Model {fitted_model_name} accuracy: {accuracy:.2f}\")\n",
        "\n",
        "    # Model balanced accuracy\n",
        "    balanced_accuracy = goodness_of_fit['balanced_accuracy']\n",
        "    print(f\"Model {fitted_model_name} balanced accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "    # Model log likelihood\n",
        "    log_likelihood = goodness_of_fit['log_likelihood']\n",
        "    print(f\"Model {fitted_model_name} log likelihood: {log_likelihood:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.5.a) We have fitted different models to some of our simulated data. Which model should we choose to explain our data? Based on what criterion? \n",
        "\n",
        "Qu.5.b) Can you find some limits to using accuracy, balanced accuracy, and log-likelihood to compare different models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Comparison \n",
        "### Why Compare Models?\n",
        "- Multiple models may explain the same behavior.\n",
        "- **Model comparison** helps us decide which model best accounts for the data.\n",
        "\n",
        "### Key Metrics\n",
        "- **Log Likelihood** (LL)\n",
        "$$\n",
        "LL = ln(P(x|\\theta,\\phi))\n",
        "$$\n",
        "- **AIC** (Akaike Information Criterion) / **BIC** (Bayesian Information Criterion): penalized likelihood criteria\n",
        "$$\n",
        "AIC = -2*(LL - n_{params})\n",
        "$$\n",
        "\n",
        "$$\n",
        "BIC = -2*(LL - \\frac{n_{params}*ln(n_{trials})}{2})\n",
        "$$\n",
        "\n",
        "- **ELBO** (evidence lower bound): \n",
        "$$\n",
        "ELBO=(Reconstruction accuracy)−(Regularization penalty)\n",
        "$$\n",
        "Maximizing ELBO improves the fit to the data (see https://pyro.ai/examples/svi_part_i.html#ELBO).\n",
        "\n",
        "### Fixed vs. Random Effects for model comparison\n",
        "- Fixed-effect analysis (FFX): assumes that a single model best describes all subjects (although subjects might still differ between each other with different model parameters).\n",
        "\n",
        "<img src=\"images\\ffxbms.jpg\" alt=\"FFX BMS\" width=\"200\"/>\n",
        "\n",
        "The same **model $m$** generated the **data $y_k$** of each **subject $k$** (possibly with different **model parameters $\\theta$ and $\\phi$**).\n",
        "\n",
        "To do a FFX model comparison:\n",
        "1. Choose your (log-) model evidence, i.e. criterion of comparison (e.g. AIC, BIC, ELBO...).\n",
        "2. For each subject, fit each model and get the corresponding model evidence (AIC, BIC, ELBO...).\n",
        "3. Sum or average the evidences over subjects for each model.\n",
        "4. Select the lowest (or highest, depending on the chosen criterion) model evidence to select the best model for the group.\n",
        "\n",
        "- Random-effect analysis (RFX): assumes that your group is drawn from a population where each model is present in different proportions, with an unknown population distribution (described in terms of model frequencies/proportions)\n",
        "\n",
        "<img src=\"images\\rfxbms.jpg\" alt=\"RFX BMS\" width=\"200\"/>\n",
        "\n",
        "Your population has a **model frequency profile $r$** (repartition of the models in the population) from which each **subject $k$** has a given **model $m_k$** and generated the **data $y_k$**. \n",
        "\n",
        "To do a RFX model comparison:\n",
        "1. Choose your (log-) model evidence, i.e. criterion of comparison (e.g. AIC, BIC, ELBO...).\n",
        "2. For each subject, fit each model and get the corresponding model evidence (AIC, BIC, ELBO...).\n",
        "3. For each subject, select the lowest (or highest, depending on the chosen criterion) model evidence to select the best model among the tested models for the subject.\n",
        "4. Compute model frequencies (frequency at which each model was selected as the best model). You can also compute model exceedance probabilities (measures how likely it is that any given model is more frequent than all other models in the comparison set, see Stephan et al., Neuroimage 2009 for more details).\n",
        "\n",
        "In summary:\n",
        "- **Fixed effects (FFX)**: assumes one best model across all participants.\n",
        "- **Random effects (RFX)**: allows for individual variability.\n",
        "\n",
        "> Tools like `pyBMS`, `SPM`, or custom Dirichlet estimation can help implement RFX comparisons.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.6.a) Fill the following to fit multiple simulated agents with all 4 models.\n",
        "\n",
        "Qu.6.b) Compute the AIC and BIC for each fitted model and subject."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fit_result = []\n",
        "nb_sbj = # TODO: Choose the number of simulated subjects to fit\n",
        "for \n",
        "    sim_hist = pd.DataFrame(full_sims_hist[k]['history'])\n",
        "    data = {'agent1_action': sim_hist['agent1_action'], 'agent2_action': sim_hist['agent2_action'], 'agent1_reward': sim_hist['agent1_reward']}\n",
        "    for fitted_model_name, (model_fn, guide_fn) in pyro_models.items():\n",
        "        print((f\"Fitting model {fitted_model_name}\"))\n",
        "\n",
        "        if(fitted_model_name==\"MIIL\"):\n",
        "            in_dict = {'game1': payoff_matrix_CG, 'game2': payoff_matrix_HaS, 'player': 1, 'dim_x': 3}\n",
        "        elif(fitted_model_name==\"Inf\"):\n",
        "            in_dict = {'game': payoff_matrix_CG, 'player': 1, 'dim_x': 1}\n",
        "        else:\n",
        "            in_dict = {'dim_x': 2}\n",
        "\n",
        "        # TODO: fill the missing inputs for fit_model() and compute_goodness_of_fit(). Explore the file CIX_course_Fitting_ToM_functions.py to see how to use these functions\n",
        "        res = fit_model( ,  ,  ,  ,  ,  , verbose=True,  )\n",
        "        goodness_of_fit = compute_goodness_of_fit( )\n",
        "        fit_result.append({'simulated_model': full_sims_hist[k]['model'], 'fitted_model': fitted_model_name, 'participant_nb': k, 'model_fit_result': res, 'goodness_of_fit': goodness_of_fit})    \n",
        "            \n",
        "        # Model accuracy\n",
        "        accuracy = goodness_of_fit['accuracy']\n",
        "        print(f\"Model {fitted_model_name} accuracy: {accuracy:.2f}\")\n",
        "\n",
        "        # Model balanced accuracy\n",
        "        balanced_accuracy = goodness_of_fit['balanced_accuracy']\n",
        "        print(f\"Model {fitted_model_name} balanced accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "        # Model log likelihood\n",
        "        log_likelihood = goodness_of_fit['log_likelihood']\n",
        "        print(f\"Model {fitted_model_name} log likelihood: {log_likelihood:.2f}\")\n",
        "\n",
        "        #TODO: compute the AIC and BIC for the fitted model\n",
        "        # Model AIC\n",
        "        aic = \n",
        "        print(f\"Model {fitted_model_name} AIC: {aic:.2f}\")\n",
        "\n",
        "        # Model BIC\n",
        "        bic = \n",
        "        print(f\"Model {fitted_model_name} BIC: {bic:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.7 Do the FFX model comparison for different criteria (AIC, BIC, ELBO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sum (or average) model evidence across participants for each fitted model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.8.a) Do the RFX model comparison based on the ELBO criterion. You might need to use some functions in CIX_course_Fitting_ToM_functions.py. \n",
        "\n",
        "Qu.8.b) Plot model comparison results for the model frequencies and exceedance probabilities. You might need to use some functions in CIX_course_Fitting_ToM_functions.py. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert ELBOs to per-participant posteriors\n",
        "\n",
        "\n",
        "# Estimate RFX model frequencies\n",
        "\n",
        "\n",
        "# Bootstrapping\n",
        "\n",
        "\n",
        "# Report\n",
        "\n",
        "\n",
        "# Plotting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sanity checks: Model recovery and parameters recovery\n",
        "\n",
        "When you build a computational model (like to explain decision-making or behavior), it’s easy to get a good-looking fit to data—but does your model actually make sense?\n",
        "\n",
        "Two important sanity checks help answer that:\n",
        "- Parameter Recovery\n",
        "- Model Recovery\n",
        "\n",
        "## Parameter recovery – “Can I trust the numbers I’m estimating?”\n",
        "\n",
        "Suppose your model has parameters (e.g., learning rate, risk aversion, influence weight, etc.). You simulate data with known parameter values, then fit your model and check:\n",
        "\n",
        "Do the fitted parameters match the ones you used?\n",
        "\n",
        "### Why does it matter?\n",
        "If your model can't recover its own parameters reliably, any interpretations you make about “high learning rate” or “low risk aversion” are not trustworthy.\n",
        "\n",
        "### How does it work?\n",
        "- Pick a model and generate synthetic data using known parameters.\n",
        "- Fit the model to that data.\n",
        "- Compare the true vs recovered parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qu.9.a) If you plot true vs recovered parameters, what should it look like if the recovery is good? Plot what a good parameter recovery should look like with fake data points.\n",
        "\n",
        "Qu.9.b) Use your previously fitted data to do a parameter recovery for each parameter of different models. Start with the QL model. How is your recovery? \n",
        "\n",
        "Qu.9.c) What happens if you change n_steps, tolerance, patience then try parameter recovery again?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#What a good parameter recovery should look like:\n",
        "# Generate true parameter values\n",
        "\n",
        "\n",
        "# Generate \"recovered\" parameter values with small noise\n",
        "\n",
        "\n",
        "# Plot true vs recovered parameters\n",
        "\n",
        "\n",
        "#How to do it with our own simulated data:\n",
        "# Organize recovery data from results and full_sims_hist\n",
        "\n",
        "\n",
        "# Generate plots\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change the guide \"guide_fn\" in the fit_model() function to None. This will set it to a default guide that is automatically set and optimized. \n",
        "\n",
        "Qu.10. What happens to the fit? To the parameter recovery? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fit_result = []\n",
        "nb_sbj = # TODO: choose the number of simulated subjects to fit and loop through them\n",
        "for \n",
        "    sim_hist = pd.DataFrame(full_sims_hist[k]['history'])\n",
        "    data = {'agent1_action': sim_hist['agent1_action'], 'agent2_action': sim_hist['agent2_action'], 'agent1_reward': sim_hist['agent1_reward']}\n",
        "    for fitted_model_name, (model_fn, guide_fn) in pyro_models.items():\n",
        "        print((f\"Fitting model {fitted_model_name}\"))\n",
        "\n",
        "        if(fitted_model_name==\"MIIL\"):\n",
        "            in_dict = {'game1': payoff_matrix_CG, 'game2': payoff_matrix_HaS, 'player': 1, 'dim_x': 3}\n",
        "        elif(fitted_model_name==\"Inf\"):\n",
        "            in_dict = {'game': payoff_matrix_CG, 'player': 1, 'dim_x': 1}\n",
        "        else:\n",
        "            in_dict = {'dim_x': 2}\n",
        "\n",
        "        # TODO: fill the missing inputs for fit_model() and compute_goodness_of_fit(). Explore the file CIX_course_Fitting_ToM_functions.py to see how to use these functions\n",
        "        res = fit_model( , , , , , , verbose=True, )\n",
        "        goodness_of_fit = compute_goodness_of_fit()\n",
        "        fit_result.append({'simulated_model': full_sims_hist[k]['model'], 'fitted_model': fitted_model_name, 'participant_nb': k, 'model_fit_result': res, 'goodness_of_fit': goodness_of_fit})    \n",
        "            \n",
        "        # Model accuracy\n",
        "        accuracy = goodness_of_fit['accuracy']\n",
        "        print(f\"Model {fitted_model_name} accuracy: {accuracy:.2f}\")\n",
        "\n",
        "        # Model balanced accuracy\n",
        "        balanced_accuracy = goodness_of_fit['balanced_accuracy']\n",
        "        print(f\"Model {fitted_model_name} balanced accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "        # Model log likelihood\n",
        "        log_likelihood = goodness_of_fit['log_likelihood']\n",
        "        print(f\"Model {fitted_model_name} log likelihood: {log_likelihood:.2f}\")\n",
        "\n",
        "        # TODO: compute the AIC and BIC for the fitted model\n",
        "        # Model AIC\n",
        "        aic = \n",
        "        print(f\"Model {fitted_model_name} AIC: {aic:.2f}\")\n",
        "\n",
        "        # Model BIC\n",
        "        bic = \n",
        "        print(f\"Model {fitted_model_name} BIC: {bic:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Organize recovery data from results and full_sims_hist\n",
        "\n",
        "\n",
        "# Generate plots\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model recovery – “Can I tell models apart?”\n",
        "\n",
        "Suppose you have two or more competing models. You simulate data using one of them, and then try to fit all the models to this simulated data. If you’re doing it right, the correct model (the one that generated the data) should be the one you recover.\n",
        "\n",
        "### Why does it matter?\n",
        "If your model comparison can't identify the right model when you already know the answer, you shouldn't trust it on real data...\n",
        "\n",
        "### How does it work?\n",
        "- Pick several models.\n",
        "- Simulate behavior from each model using known parameters.\n",
        "- Fit all models to that simulated data.\n",
        "- See if you correctly identify the model that generated the data.\n",
        "- If not → data generated by your models is too similar, or the fitting procedure is flawed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To represent a model recovery, we usually plot the confusion matrix showing how often the right model is chosen. For each simulated model, we plot the estimated frequencies after fitting all models. \n",
        "\n",
        "Qu.11.a) If we have 4 models (e.g. the QL, FP, Influence and MIIL models), how would this matrix look like if the model recovery is good? Plot a dummy recovery matrix that would represent a good model recovery.\n",
        "\n",
        "Qu.11.b) Plot a dummy matrix that would represent a bad model recovery. What is wrong with your matrix? What would it mean in terms of your fitting procedure or models? \n",
        "\n",
        "Qu.12.c) Think about what you can possibly do to improve model recovery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up dummy 4x4 confusion matrix\n",
        "# Good recovery matrix\n",
        "\n",
        "\n",
        "# Bad recovery matrix \n",
        "\n",
        "\n",
        "# Model names\n",
        "\n",
        "\n",
        "# Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMCIZ3P0bIXf"
      },
      "source": [
        "Ideally, we would like to perform a model recovery on our simulated data. However, the fitting procedure will take a very long time. It would need parallelization. So this will be for another time.\n",
        "\n",
        "End of session."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
